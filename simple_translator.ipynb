{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOCcw0pyUBWFI6Urx9fHLVk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Darian-Lee-YTKA/RNN-Russian-Translator/blob/main/simple_translator.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pymorphy2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qeirRnJx525-",
        "outputId": "6da90766-3016-4e13-b645-1d55e1bf82d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pymorphy2\n",
            "  Downloading pymorphy2-0.9.1-py3-none-any.whl (55 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.5/55.5 kB\u001b[0m \u001b[31m825.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting dawg-python>=0.7.1 (from pymorphy2)\n",
            "  Downloading DAWG_Python-0.7.2-py2.py3-none-any.whl (11 kB)\n",
            "Collecting pymorphy2-dicts-ru<3.0,>=2.4 (from pymorphy2)\n",
            "  Downloading pymorphy2_dicts_ru-2.4.417127.4579844-py2.py3-none-any.whl (8.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.2/8.2 MB\u001b[0m \u001b[31m26.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting docopt>=0.6 (from pymorphy2)\n",
            "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: docopt\n",
            "  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13706 sha256=8f03b6e484dbe4bcf1bff75beeef671e224a7d45ac954b83aebef01b4466f811\n",
            "  Stored in directory: /root/.cache/pip/wheels/fc/ab/d4/5da2067ac95b36618c629a5f93f809425700506f72c9732fac\n",
            "Successfully built docopt\n",
            "Installing collected packages: pymorphy2-dicts-ru, docopt, dawg-python, pymorphy2\n",
            "Successfully installed dawg-python-0.7.2 docopt-0.6.2 pymorphy2-0.9.1 pymorphy2-dicts-ru-2.4.417127.4579844\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iOk3lt0k2ZSN",
        "outputId": "0cfd614f-1464-4c30-b8e8-5632226517fa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import pymorphy2\n",
        "import random\n",
        "import nltk\n",
        "import re\n",
        "import string\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt')\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "with open('/content/rus.txt', 'r') as file:\n",
        "    content = file.readlines()\n",
        "\n",
        "def remove_punct(line):\n",
        "    line = line.replace(\"n't\", \" not\") # so conjuctions can be tokenized more easily\n",
        "    line = line.replace(\"'m\", \" am\")\n",
        "    line = line.replace(\"let's\", \"let us\")\n",
        "    line = line.replace(\"'re\", \" are\")\n",
        "    line = line.replace(\"Tom's\", \"Toms\") # so possestive doesn't become \"is\"\n",
        "    line = line.replace(\"'s\", \" is\")\n",
        "    line = line.replace(\"'ve\", \" have\")\n",
        "    line = line.replace(\"'ll\", \" will\")\n",
        "    line = line.replace(\"won't\", \" will not\")\n",
        "    line = line.replace(\"'\", \" \")\n",
        "\n",
        "    # I am going to keep the question make in since it effects word order\n",
        "    punct_except_question_mark = ''.join(char for char in string.punctuation if char != '?')\n",
        "    punct_regex = re.compile('[%s]' % re.escape(punct_except_question_mark))\n",
        "    line = punct_regex.sub('', line)\n",
        "    return line\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "russian_list = []\n",
        "english_list = []\n",
        "\n",
        "# the lines are grouped by complexity.\n",
        "\n",
        "\n",
        "for line in content[:25000]: #only do the first 25,000 to keep the model a more managable size\n",
        "\n",
        "# NOTE: this size will likely shrink even more once we delete translations that become duplicates once the Russian is lematized\n",
        "\n",
        "    line = re.sub(r'\\tCC-BY.*', '', line) #get rid of everything that is not data\n",
        "    line = re.sub(r'\\t', 'β', line) #make the lines easier to split. I was getting errors doing line.split based on tab,\n",
        "    # so I replaced the tabs with beta\n",
        "\n",
        "    line = remove_punct(line)\n",
        "\n",
        "    eng, rus = line.split('β')\n",
        "\n",
        "    rus = rus.strip().lower()  #to get rid of /n at end and make it all the same case\n",
        "\n",
        "    eng = eng.strip().lower()\n",
        "\n",
        "    russian_list.append(rus+' ')  #this will come in handy later when we add 'p' to tag past tense\n",
        "\n",
        "    english_list.append(eng)\n",
        "\n",
        "\n",
        "\n",
        "data = pd.DataFrame({'rus': russian_list, 'eng': english_list})\n",
        "data = data.sample(frac=1, random_state=12) #shuffle it\n",
        "russian_list = data['rus'].tolist()\n",
        "english_list = data['eng'].tolist()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uHB3HC3mhbak",
        "outputId": "bb1dc866-fd59-4af0-ab28-511f7f25f745"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(25000, 2)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install spacy\n",
        "!python -m spacy download ru_core_news_sm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kkJel65bEpp3",
        "outputId": "d6f5a042-e4e8-4d53-e87b-eb31b8c5b92e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.10/dist-packages (3.6.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy) (8.1.12)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.9.0)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.10.3)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (4.66.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.23.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.10.13)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (23.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.3.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2023.11.17)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.1.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy) (8.1.7)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy) (2.1.3)\n",
            "2024-01-02 20:23:14.860642: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-01-02 20:23:14.860736: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-01-02 20:23:14.862310: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-01-02 20:23:16.403077: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Collecting ru-core-news-sm==3.6.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/ru_core_news_sm-3.6.0/ru_core_news_sm-3.6.0-py3-none-any.whl (15.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.3/15.3 MB\u001b[0m \u001b[31m35.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.7.0,>=3.6.0 in /usr/local/lib/python3.10/dist-packages (from ru-core-news-sm==3.6.0) (3.6.1)\n",
            "Collecting pymorphy3>=1.0.0 (from ru-core-news-sm==3.6.0)\n",
            "  Downloading pymorphy3-2.0.0-py3-none-any.whl (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m774.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: dawg-python>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from pymorphy3>=1.0.0->ru-core-news-sm==3.6.0) (0.7.2)\n",
            "Collecting pymorphy3-dicts-ru (from pymorphy3>=1.0.0->ru-core-news-sm==3.6.0)\n",
            "  Downloading pymorphy3_dicts_ru-2.4.417150.4580142-py2.py3-none-any.whl (8.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.4/8.4 MB\u001b[0m \u001b[31m28.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->ru-core-news-sm==3.6.0) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->ru-core-news-sm==3.6.0) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->ru-core-news-sm==3.6.0) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->ru-core-news-sm==3.6.0) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->ru-core-news-sm==3.6.0) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->ru-core-news-sm==3.6.0) (8.1.12)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->ru-core-news-sm==3.6.0) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->ru-core-news-sm==3.6.0) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->ru-core-news-sm==3.6.0) (2.0.10)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->ru-core-news-sm==3.6.0) (0.9.0)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->ru-core-news-sm==3.6.0) (0.10.3)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->ru-core-news-sm==3.6.0) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->ru-core-news-sm==3.6.0) (4.66.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->ru-core-news-sm==3.6.0) (1.23.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->ru-core-news-sm==3.6.0) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->ru-core-news-sm==3.6.0) (1.10.13)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->ru-core-news-sm==3.6.0) (3.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->ru-core-news-sm==3.6.0) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->ru-core-news-sm==3.6.0) (23.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->ru-core-news-sm==3.6.0) (3.3.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.7.0,>=3.6.0->ru-core-news-sm==3.6.0) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->ru-core-news-sm==3.6.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->ru-core-news-sm==3.6.0) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->ru-core-news-sm==3.6.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->ru-core-news-sm==3.6.0) (2023.11.17)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->ru-core-news-sm==3.6.0) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->ru-core-news-sm==3.6.0) (0.1.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy<3.7.0,>=3.6.0->ru-core-news-sm==3.6.0) (8.1.7)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.7.0,>=3.6.0->ru-core-news-sm==3.6.0) (2.1.3)\n",
            "Installing collected packages: pymorphy3-dicts-ru, pymorphy3, ru-core-news-sm\n",
            "Successfully installed pymorphy3-2.0.0 pymorphy3-dicts-ru-2.4.417150.4580142 ru-core-news-sm-3.6.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('ru_core_news_sm')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(russian_list)): #for tagging past tense\n",
        "# in russian, past tense is super easy to detect for most words because it will have one of these endings\n",
        "    russian_list[i] = re.sub(r'л(?=\\s|$)', 'л p', russian_list[i])\n",
        "    russian_list[i] = re.sub(r'ла(?=\\s|$)', 'ла p', russian_list[i])\n",
        "    russian_list[i] = re.sub(r'лся(?=\\s|$)', 'лся p', russian_list[i])\n",
        "    russian_list[i] = re.sub(r'лась(?=\\s|$)', 'лась p', russian_list[i])\n",
        "    russian_list[i] = re.sub(r'ло(?=\\s|$)', 'ло p', russian_list[i])\n",
        "    russian_list[i] = re.sub(r'лось(?=\\s|$)', 'лось p', russian_list[i])\n",
        "    russian_list[i] = re.sub(r'ли(?=\\s|$)', 'ли p', russian_list[i])\n",
        "    russian_list[i] = re.sub(r'лись(?=\\s|$)', 'лись p', russian_list[i])\n",
        "    russian_list[i] = re.sub(r'\\s+$', '', russian_list[i], flags=re.MULTILINE)\n"
      ],
      "metadata": {
        "id": "_Q_5NVWsGHCU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "russian_list"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ezfR2hF5Goky",
        "outputId": "01db651b-246c-414f-e085-221e035c6cb6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['я очень стара',\n",
              " 'я зажатый',\n",
              " 'не приходите сюда',\n",
              " 'оно близко',\n",
              " 'я обожаю тебя',\n",
              " 'заедьте за ним',\n",
              " 'он умеет читать',\n",
              " 'том посмотрел p',\n",
              " 'спрячьтесь здесь',\n",
              " 'тебе том нравится?',\n",
              " 'кажется ты хороший',\n",
              " 'мыслите позитивно',\n",
              " 'звоните в любое время',\n",
              " 'ты пьян?',\n",
              " 'теперь припоминаю',\n",
              " 'наелись?',\n",
              " 'подожди здесь',\n",
              " 'не облажайся',\n",
              " 'я парень занятой',\n",
              " 'ты это отрицаешь?',\n",
              " 'мне понравилось p',\n",
              " 'теперь проваливай',\n",
              " 'давайте быстрей',\n",
              " 'я скучаю по тому',\n",
              " 'можно мне с вами?',\n",
              " 'я голосую против',\n",
              " 'мне пришлось p подождать',\n",
              " 'не беспокой меня',\n",
              " 'это нам решать',\n",
              " 'том облажался p',\n",
              " 'сейчас я бодрствую',\n",
              " 'у тебя бледный вид',\n",
              " 'его там нет',\n",
              " 'дома есть ктонибудь?',\n",
              " 'не переусердствуй',\n",
              " 'у меня грипп',\n",
              " 'попробуй сделай это',\n",
              " 'будь благоразумен',\n",
              " 'сахар сладкий',\n",
              " 'она ленивая',\n",
              " 'я сожалею об этом',\n",
              " 'я не силён',\n",
              " 'будь очень осторожна',\n",
              " 'я ещё приду',\n",
              " 'я раньше курил p',\n",
              " 'том был p ранен?',\n",
              " 'мы подали p на них в суд',\n",
              " 'это безумие',\n",
              " 'они были p мертвы',\n",
              " 'это не так',\n",
              " 'их губы встретились p',\n",
              " 'я стрелял p в тома',\n",
              " 'дай сказать',\n",
              " 'нам надо поговорить',\n",
              " 'я встал p рано',\n",
              " 'попробуйте того',\n",
              " 'тебе удаётся спать?',\n",
              " 'мы расстались p',\n",
              " 'том никого не видел p',\n",
              " 'какиенибудь предложения?',\n",
              " 'том умеет готовить?',\n",
              " 'этого достаточно',\n",
              " 'ты пьяная',\n",
              " 'огонь',\n",
              " 'я знаю свою работу',\n",
              " 'я удивлен',\n",
              " 'ты поможешь?',\n",
              " 'том плюнул p',\n",
              " 'мы победили p',\n",
              " 'он слишком короткий',\n",
              " 'кто уволился?',\n",
              " 'у меня получится',\n",
              " 'я пойду следующим',\n",
              " 'свари рис',\n",
              " 'я надел p парик',\n",
              " 'я чувствовал p себя ужасно',\n",
              " 'можно мне посмотреть?',\n",
              " 'том набрал p скорость',\n",
              " 'вы мерзавец',\n",
              " 'мы победители p',\n",
              " 'никто не видел p тома',\n",
              " 'мы будем сражаться',\n",
              " 'вы меня надули p',\n",
              " 'мы нуждаемся в тебе',\n",
              " 'вы с нами?',\n",
              " 'досчитай до тридцати',\n",
              " 'дай мне полотенце',\n",
              " 'ты побеждаешь',\n",
              " 'сидите дома',\n",
              " 'я включил p плиту',\n",
              " 'я простыл p',\n",
              " 'том дурак',\n",
              " 'я спал p',\n",
              " 'я встала p в очередь',\n",
              " 'это требует времени',\n",
              " 'он не умеет водить',\n",
              " 'он был p моим начальником',\n",
              " 'стыдитесь',\n",
              " 'вы аморальны',\n",
              " 'больно?',\n",
              " 'я глухая',\n",
              " 'иди в парк',\n",
              " 'я поменяю это',\n",
              " 'том в сети',\n",
              " 'том ослабевший',\n",
              " 'слишком большой?',\n",
              " 'это не было p правдой',\n",
              " 'спроси меня',\n",
              " 'давай',\n",
              " 'во сколько примерно?',\n",
              " 'я выращиваю рис',\n",
              " 'спроси тома',\n",
              " 'он исполнил p свой долг',\n",
              " 'том играл p',\n",
              " 'том переехал p',\n",
              " 'прости тома',\n",
              " 'я бы купил p это',\n",
              " 'я ничего не боюсь',\n",
              " 'не смотрите',\n",
              " 'прочитайте это вслух',\n",
              " 'я обожаю кино',\n",
              " 'уведомите меня',\n",
              " 'том ворвался p',\n",
              " 'давай сделаем это',\n",
              " 'закрой книгу',\n",
              " 'я вижу образец',\n",
              " 'деньги  сила p',\n",
              " 'я дважды проверил p',\n",
              " 'я потел p',\n",
              " 'оставь его здесь',\n",
              " 'мы сделаем это',\n",
              " 'не бойся',\n",
              " 'не ходи том',\n",
              " 'я временный работник',\n",
              " 'том был p не согласен',\n",
              " 'я нашла p тома',\n",
              " 'давай поплаваем',\n",
              " 'ты не можешь выиграть',\n",
              " 'я не придирчив',\n",
              " 'замолчи',\n",
              " 'я возьму с собой тома',\n",
              " 'я тоже такое хочу',\n",
              " 'я буду свободна',\n",
              " 'вы его слышали?',\n",
              " 'вы согласились p',\n",
              " 'он молодой',\n",
              " 'я вас там видел p',\n",
              " 'я не застряла p',\n",
              " 'будь терпелива',\n",
              " 'оно дома',\n",
              " 'я опоздал?',\n",
              " 'лови',\n",
              " 'ему исполнилось p шестнадцать',\n",
              " 'кто играет в гольф?',\n",
              " 'могу я начать?',\n",
              " 'тебе бы понравилось p',\n",
              " 'том мне помог',\n",
              " 'давай начнём',\n",
              " 'я была p одинока',\n",
              " 'нам нужно такси',\n",
              " 'я люблю гарвард',\n",
              " 'ты умеешь водить машину?',\n",
              " 'я не ошиблась p',\n",
              " 'это порок',\n",
              " 'мне очень грустно',\n",
              " 'я её починил p',\n",
              " 'он восхищался p ей',\n",
              " 'давай его подвинем',\n",
              " 'отвернитесь',\n",
              " 'вы бедны',\n",
              " 'полагаю что нет',\n",
              " 'он доверял p тебе',\n",
              " 'том заходил?',\n",
              " 'бросьте его туда',\n",
              " 'он шагает быстро',\n",
              " 'меня избили p',\n",
              " 'я не умею свистеть',\n",
              " 'заходи',\n",
              " 'я всё ещё злюсь',\n",
              " 'я в одиночестве',\n",
              " 'это понедельник?',\n",
              " 'вошёл p том',\n",
              " 'идите',\n",
              " 'это изменится',\n",
              " 'мы не поём',\n",
              " 'мы богаты',\n",
              " 'мне нехорошо',\n",
              " 'я  ветеран',\n",
              " 'мне входить?',\n",
              " 'тут я начальник',\n",
              " 'боишься?',\n",
              " 'я могу сделать это сейчас',\n",
              " 'я глухой',\n",
              " 'ешьте больше фруктов',\n",
              " 'толкайте сильнее',\n",
              " 'том стрелял p в него',\n",
              " 'открывайте огонь',\n",
              " 'том юн?',\n",
              " 'вы бесполезны',\n",
              " 'давай ей поможем',\n",
              " 'продолжай сражаться',\n",
              " 'кто ей помогает?',\n",
              " 'они расстроены',\n",
              " 'мне пришлось p работать',\n",
              " 'я спал p на улице',\n",
              " 'он в ударе',\n",
              " 'ты забавная',\n",
              " 'я позвоню',\n",
              " 'никогда не сдавайтесь',\n",
              " 'том приблизил p вид',\n",
              " 'том обожает меня',\n",
              " 'иди потуда',\n",
              " 'том меня любит',\n",
              " 'мы тебя слышали p',\n",
              " 'хватит петь',\n",
              " 'мне нравятся абрикосы',\n",
              " 'напитки бесплатные?',\n",
              " 'тома оштрафовали p',\n",
              " 'я убеждена',\n",
              " 'исправь его пожалуйста',\n",
              " 'я не могу опаздывать',\n",
              " 'это выполнимо',\n",
              " 'ты бы развлёкся',\n",
              " 'сделайте это для тома',\n",
              " 'том может умереть',\n",
              " 'я сделал p всё от меня зависящее',\n",
              " 'он это заслужил p',\n",
              " 'они все уехали p',\n",
              " 'сработало p',\n",
              " 'кто помог тому?',\n",
              " 'сейчас твоя очередь',\n",
              " 'вы все сумасшедшие',\n",
              " 'готово',\n",
              " 'я могу объяснить',\n",
              " 'каким был p обед?',\n",
              " 'мне тридцать лет',\n",
              " 'ты шпионка?',\n",
              " 'у меня были p свои сомнения',\n",
              " 'все по местам',\n",
              " 'я не плаваю',\n",
              " 'мы голодные',\n",
              " 'я видел p козла p',\n",
              " 'теперь попробуйте снова',\n",
              " 'том присутствует',\n",
              " 'я хочу свой',\n",
              " 'мы не заняты',\n",
              " 'у меня икота',\n",
              " 'я войду внутрь',\n",
              " 'она у тебя?',\n",
              " 'ничего страшного',\n",
              " 'поешьте ветчины',\n",
              " 'можете тоже пойти?',\n",
              " 'круто',\n",
              " 'я плавал p',\n",
              " 'я кивнула p',\n",
              " 'будет трудно',\n",
              " 'будем счастливы',\n",
              " 'исчезни',\n",
              " 'подожди секундочку',\n",
              " 'я пойду сама',\n",
              " 'можешь её найти?',\n",
              " 'я не был p пьяный',\n",
              " 'ты мой',\n",
              " 'кто её купит?',\n",
              " 'я выгляжу уставшим?',\n",
              " 'я очень слаб',\n",
              " 'будь беспощаден',\n",
              " 'есть кто?',\n",
              " 'ты можешь идти?',\n",
              " 'похлопайте пожалуйста',\n",
              " 'том по тебе скучал p',\n",
              " 'это был p слух',\n",
              " 'не холодно',\n",
              " 'мы вас знаем',\n",
              " 'это наверное том',\n",
              " 'я готовлю чай',\n",
              " 'у него сын болеет',\n",
              " 'это старомодно',\n",
              " 'я могу позвать тома',\n",
              " 'это наша',\n",
              " 'я знаю песню',\n",
              " 'у тома получилось p',\n",
              " 'умираю есть хочу',\n",
              " 'вы нас напугали p',\n",
              " 'я пенсионер',\n",
              " 'это я виноват?',\n",
              " 'я у него попрошу',\n",
              " 'найди немного времени',\n",
              " 'меня не считайте',\n",
              " 'спрячь эту книгу',\n",
              " 'в тома стреляли p',\n",
              " 'они выиграли p',\n",
              " 'я вызвала p такси',\n",
              " 'не беги',\n",
              " 'том крут',\n",
              " 'я попыталась p снова',\n",
              " 'перестаньте болтать',\n",
              " 'том умер',\n",
              " 'я возмущён',\n",
              " 'это бесплатно',\n",
              " 'мне нравится ходить пешком',\n",
              " 'дай нам какуюнибудь подсказку',\n",
              " 'я люблю вас обоих',\n",
              " 'том тяжёлый',\n",
              " 'я его не вижу',\n",
              " 'том так сказал p',\n",
              " 'ты свободен',\n",
              " 'здесь никого нет',\n",
              " 'возвращайся поскорее',\n",
              " 'это я',\n",
              " 'мне пришлось p это сделать',\n",
              " 'счастливого хэллоуина',\n",
              " 'том умеет водить машину',\n",
              " 'я не ребёнок',\n",
              " 'не мешай мне',\n",
              " 'позволь мне уйти',\n",
              " 'подпевайте',\n",
              " 'ты постишься?',\n",
              " 'я испекла p печенье',\n",
              " 'забавно',\n",
              " 'я не могу залогиниться',\n",
              " 'мы попробовали p',\n",
              " 'оставьте её там',\n",
              " 'ну же дотронься',\n",
              " 'пора',\n",
              " 'я изменился p',\n",
              " 'мне закрывать?',\n",
              " 'я большая девочка',\n",
              " 'ты первый',\n",
              " 'спросите у когонибудь',\n",
              " 'ты это подписывал?',\n",
              " 'отец занят',\n",
              " 'я люблю красный перец',\n",
              " 'том занимается йогой',\n",
              " 'есть кто дома?',\n",
              " 'ты плачешь?',\n",
              " 'она моя',\n",
              " 'давай их спросим',\n",
              " 'мне нужна была p еда',\n",
              " 'я ещё могу победить',\n",
              " 'мы ждём',\n",
              " 'я очень ленивая',\n",
              " 'я предупредил p тома',\n",
              " 'мы были p согласны',\n",
              " 'берите фотоаппарат',\n",
              " 'том беден',\n",
              " 'я скучал p по вам',\n",
              " 'мы робкие',\n",
              " 'я увидела p автобус',\n",
              " 'сделаете?',\n",
              " 'я должен помочь',\n",
              " 'спроси тома ещё раз',\n",
              " 'увидимся позже',\n",
              " 'том может ходить',\n",
              " 'я проиграл p пари',\n",
              " 'они поссорились p',\n",
              " 'вы чокнутые',\n",
              " 'терпеть не могу понедельники',\n",
              " 'том на дежурстве?',\n",
              " 'я не умею петь',\n",
              " 'я спросил p тома',\n",
              " 'я обвинял p себя',\n",
              " 'я взошёл p на борт',\n",
              " 'поправляйтесь скорее',\n",
              " 'горячо',\n",
              " 'давай поменяемся',\n",
              " 'я репортёр',\n",
              " 'я не молода',\n",
              " 'не зови меня',\n",
              " 'я переехал p',\n",
              " 'можно я спою?',\n",
              " 'вот и мы',\n",
              " 'я ничего не увидела p',\n",
              " 'вот и они',\n",
              " 'давай спросим',\n",
              " 'я должен выяснить',\n",
              " 'мы приятели p',\n",
              " 'мне он ещё нужен',\n",
              " 'я могу с этим справиться',\n",
              " 'я должно быть пьян',\n",
              " 'вы быстро говорите',\n",
              " 'прочтите эту книгу',\n",
              " 'я так устал p',\n",
              " 'я никогда вас не видела p',\n",
              " 'я люблю осень',\n",
              " 'будьте благоразумны',\n",
              " 'я читаю книгу',\n",
              " 'том заикался p',\n",
              " 'не мешайте мне',\n",
              " 'мне нужны ключи',\n",
              " 'это умно',\n",
              " 'вкусно',\n",
              " 'я зажёг свечу',\n",
              " 'том коп',\n",
              " 'том ревнует',\n",
              " 'том умеет кататься на коньках',\n",
              " 'том большой?',\n",
              " 'я болею',\n",
              " 'не уроните его',\n",
              " 'том затормозил p',\n",
              " 'помогите мне его поднять',\n",
              " 'я могу начинать?',\n",
              " 'ты что пьяна?',\n",
              " 'том здесь плавает',\n",
              " 'мы поладили p',\n",
              " 'том хранил p молчание',\n",
              " 'он заставил p меня поехать',\n",
              " 'том аккуратный',\n",
              " 'пошли p',\n",
              " 'он не идёт',\n",
              " 'это нелегко',\n",
              " 'мне нечего есть',\n",
              " 'я не спал p',\n",
              " 'дайте мне пончик',\n",
              " 'мы можем спеть?',\n",
              " 'том выглядит больным',\n",
              " 'мы пойдём',\n",
              " 'ты можешь это сделать',\n",
              " 'у него плохое зрение',\n",
              " 'она может сломаться',\n",
              " 'иди домой быстро',\n",
              " 'это я',\n",
              " 'подай мне полотенце',\n",
              " 'целься огонь',\n",
              " 'не могу этого отрицать',\n",
              " 'сделай это пожалуйста',\n",
              " 'иди к нам',\n",
              " 'посмотри на него',\n",
              " 'подождите секундочку',\n",
              " 'ты мне нравишься том',\n",
              " 'не рискуй',\n",
              " 'я не стеснительная',\n",
              " 'вы расслаблены?',\n",
              " 'они пьяны',\n",
              " 'виновата',\n",
              " 'вы нас надули p',\n",
              " 'она разбита?',\n",
              " 'я знаю эту местность',\n",
              " 'я сделала p это сама',\n",
              " 'он том?',\n",
              " 'дай огоньку',\n",
              " 'я за это заплатила p',\n",
              " 'это ваш сын',\n",
              " 'я действительно их люблю',\n",
              " 'вы её взяли?',\n",
              " 'что за день',\n",
              " 'иди туда',\n",
              " 'том может победить',\n",
              " 'танцуйте дальше',\n",
              " 'завязывай с азартными играми',\n",
              " 'я довёл p их до слёз',\n",
              " 'уйди пожалуйста',\n",
              " 'я одного видел p',\n",
              " 'вас пригласили p',\n",
              " 'я серьёзно ранен',\n",
              " 'ты маленький',\n",
              " 'я хранил p молчание',\n",
              " 'разбей это',\n",
              " 'теперь убирайтесь',\n",
              " 'я любила p вас',\n",
              " 'ему было p больно',\n",
              " 'вали p отсюда',\n",
              " 'я доступен',\n",
              " 'никто не умер',\n",
              " 'я занервничал p',\n",
              " 'начинайте заново',\n",
              " 'кто их нашёл?',\n",
              " 'можно я вас поцелую?',\n",
              " 'я подмигнул p в ответ',\n",
              " 'у тебя есть вино',\n",
              " 'том должен это сделать',\n",
              " 'я немного опаздываю',\n",
              " 'правда что ли?',\n",
              " 'просто держись подальше',\n",
              " 'мы безвредные',\n",
              " 'соответствуй возрасту',\n",
              " 'как прошло?',\n",
              " 'они оба ушли p',\n",
              " 'попробуй ту',\n",
              " 'он сломан?',\n",
              " 'мне надо пойти одному',\n",
              " 'у нас есть машина',\n",
              " 'прикрой ноги',\n",
              " 'ни в коем случае',\n",
              " 'я встретился p с томом',\n",
              " 'том ужасен',\n",
              " 'до сих пор болит',\n",
              " 'ты должна поесть',\n",
              " 'вы больны?',\n",
              " 'я вполне уверен',\n",
              " 'я боюсь',\n",
              " 'мэри нравилась p тому',\n",
              " 'я из токио',\n",
              " 'ты со мной?',\n",
              " 'мам я дома',\n",
              " 'хорошего тебе дня',\n",
              " 'оно чистое',\n",
              " 'мы сможем сделать это?',\n",
              " 'покойся с миром',\n",
              " 'вам нехорошо?',\n",
              " 'можете прийти?',\n",
              " 'том ухмыляется',\n",
              " 'ты лысая?',\n",
              " 'дорогу я знаю',\n",
              " 'это пари',\n",
              " 'бывает',\n",
              " 'я твоя',\n",
              " 'ты понравился p тому',\n",
              " 'ты ранена?',\n",
              " 'вскипятите воду',\n",
              " 'том не пел?',\n",
              " 'мы помогаем тому',\n",
              " 'принеси мне мою шляпу',\n",
              " 'одного раза достаточно',\n",
              " 'я сам это сделал p',\n",
              " 'докажи это',\n",
              " 'мы ковбои',\n",
              " 'вам она понадобится',\n",
              " 'он слабоумный',\n",
              " 'ты разве не видишь?',\n",
              " 'где он?',\n",
              " 'он был p тихим',\n",
              " 'я была p пристыжена',\n",
              " 'берите мои',\n",
              " 'я болен',\n",
              " 'ненавижу ждать',\n",
              " 'я ему помогаю',\n",
              " 'закройте книгу',\n",
              " 'я слышала p это',\n",
              " 'том меня проигнорировал p',\n",
              " 'мы подчинимся',\n",
              " 'не отступайте',\n",
              " 'они встали p',\n",
              " 'кто сказал p да?',\n",
              " 'я ненавижу оперу',\n",
              " 'я пока что в порядке',\n",
              " 'вам надо бы поехать',\n",
              " 'можешь уйти',\n",
              " 'унесите его',\n",
              " 'том молодой',\n",
              " 'веди медленно',\n",
              " 'насколько он широкий?',\n",
              " 'бди',\n",
              " 'кто поймал p тома?',\n",
              " 'они его потеряли p',\n",
              " 'спросите тома',\n",
              " 'я хочу чтобы вы вернулись p',\n",
              " 'он невиновен',\n",
              " 'выпустите меня',\n",
              " 'я их люблю',\n",
              " 'я не властная',\n",
              " 'я не при исполнении',\n",
              " 'я не ошибаюсь',\n",
              " 'попробуйте это',\n",
              " 'стойте неподвижно',\n",
              " 'оно нужно мне сегодня',\n",
              " 'так и есть',\n",
              " 'мы обречены',\n",
              " 'сходи за кофе',\n",
              " 'я старый',\n",
              " 'я человек',\n",
              " 'что изменится?',\n",
              " 'я снова виделся p с томом',\n",
              " 'мы вас любим',\n",
              " 'я буду стрелять',\n",
              " 'кого вы видите?',\n",
              " 'том меня впустил p',\n",
              " 'мы тебя найдём',\n",
              " 'нам нужен дождь',\n",
              " 'том голодный?',\n",
              " 'она рада',\n",
              " 'давайте его продадим',\n",
              " 'а ну прекратите',\n",
              " 'это вымысел?',\n",
              " 'я его вернул p',\n",
              " 'я только улыбнулся p',\n",
              " 'мы в безопасности?',\n",
              " 'том был p грубым',\n",
              " 'он спит',\n",
              " 'я могу тебе доверять?',\n",
              " 'мыслите позитивно',\n",
              " 'том  святой',\n",
              " 'не переживай',\n",
              " 'французский язык лёгкий?',\n",
              " 'он добр ко мне',\n",
              " 'пусто',\n",
              " 'будем на связи',\n",
              " 'я не запла́чу',\n",
              " 'это хлопок',\n",
              " 'я вас убью',\n",
              " 'я пойду в любом случае',\n",
              " 'иди обратно внутрь',\n",
              " 'я забывчивый',\n",
              " 'я беспощаден',\n",
              " 'давайте поспешим',\n",
              " 'мы ревнуем',\n",
              " 'я спряталась p',\n",
              " 'прекратите драться',\n",
              " 'вот попробуй',\n",
              " 'я люблю поболтать',\n",
              " 'я может быть вернусь',\n",
              " 'я в форме',\n",
              " 'наш кот толстый',\n",
              " 'я обожаю покер',\n",
              " 'они симпатичные?',\n",
              " 'том нормальный',\n",
              " 'у меня есть работа',\n",
              " 'я тут один?',\n",
              " 'я не робкая',\n",
              " 'хорошо я согласна',\n",
              " 'это имеет значение?',\n",
              " 'я с ней поговорил p',\n",
              " 'ты это видишь?',\n",
              " 'не напивайтесь',\n",
              " 'ты подождал?',\n",
              " 'мячи круглые',\n",
              " 'я должна уехать',\n",
              " 'мы тебя любим',\n",
              " 'постарайся сосредоточиться',\n",
              " 'обязательно приходите ещё',\n",
              " 'это пластмасса',\n",
              " 'некоторые рыбы летают',\n",
              " 'наденьте это',\n",
              " 'вызови мне такси',\n",
              " 'он стоил p тридцать долларов',\n",
              " 'прекрати ныть',\n",
              " 'мы можем уйти',\n",
              " 'она сейчас не здесь',\n",
              " 'я пас',\n",
              " 'больно',\n",
              " 'попросите лучше тома',\n",
              " 'верь мне',\n",
              " 'я хочу собаку',\n",
              " 'том поможет',\n",
              " 'как грубо',\n",
              " 'вас удочерили p',\n",
              " 'не ждите',\n",
              " 'ужасно',\n",
              " 'я бы сделал p это',\n",
              " 'накройте на стол p',\n",
              " 'это была p моя вина',\n",
              " 'я вернулась p',\n",
              " 'мы приехали p',\n",
              " 'у нас получилось p',\n",
              " 'он прикусил p губу',\n",
              " 'я справляюсь',\n",
              " 'кто вы?',\n",
              " 'они увидели p меня',\n",
              " 'том кроткий',\n",
              " 'вы нечто',\n",
              " 'том улыбается?',\n",
              " 'делай что я говорю',\n",
              " 'теперь оно ваше',\n",
              " 'я нормальный',\n",
              " 'ты несчастен?',\n",
              " 'вкусно пахнет',\n",
              " 'у меня сердце болит',\n",
              " 'мне нужна твоя машина',\n",
              " 'никто не шелохнулся p',\n",
              " 'том дома',\n",
              " 'не обращайте на это внимания',\n",
              " 'кто хочет пить?',\n",
              " 'я нанял p тома',\n",
              " 'том внутри',\n",
              " 'был p вечер',\n",
              " 'ты её поменял?',\n",
              " 'я сочиняю песни',\n",
              " 'давай ему поможем',\n",
              " 'не может быть',\n",
              " 'никому не доверяйте',\n",
              " 'я взволнован',\n",
              " 'вы богатый?',\n",
              " 'том укусил p мэри',\n",
              " 'оставь меня в покое',\n",
              " 'я был p в депрессии',\n",
              " 'они вас видели p',\n",
              " 'вылезай из постели p',\n",
              " 'сожги тело p',\n",
              " 'хорошо давай',\n",
              " 'я почувствовал p облегчение',\n",
              " 'он мне нравится',\n",
              " 'том будет драться',\n",
              " 'ты его поменял?',\n",
              " 'не приходите',\n",
              " 'включи её',\n",
              " 'покажите это мне',\n",
              " 'он был p большой',\n",
              " 'вы себя нормально чувствуете?',\n",
              " 'никто не улыбнулся p',\n",
              " 'это французский?',\n",
              " 'план таков',\n",
              " 'возьмите только одну',\n",
              " 'я отлично себя чувствую',\n",
              " 'это может сработать',\n",
              " 'это волк?',\n",
              " 'попробуйте ещё раз',\n",
              " 'у меня двойняшки',\n",
              " 'делай что хочешь',\n",
              " 'они его купили p',\n",
              " 'том врёт',\n",
              " 'попробуй ещё раз',\n",
              " 'завяжи шнурки на ботинках',\n",
              " 'я вернулся p',\n",
              " 'кто поедет?',\n",
              " 'я кивнул p в знак согласия',\n",
              " 'не обращайте на тома внимания',\n",
              " 'скучаешь по тому?',\n",
              " 'я не могу им пользоваться',\n",
              " 'я пошёл p за томом',\n",
              " 'том вдохнул p',\n",
              " 'заканчивай работу',\n",
              " 'я вся твоя',\n",
              " 'как у вас дела?',\n",
              " 'у меня есть осёл p',\n",
              " 'я замёрзла p',\n",
              " 'мы беспристрастны',\n",
              " 'я нахмурился p',\n",
              " 'я проигнорировал p тома',\n",
              " 'возвращайся',\n",
              " 'вы едите мясо?',\n",
              " 'он высокий?',\n",
              " 'расскажи всем и каждому',\n",
              " 'кто отменил?',\n",
              " 'это вино',\n",
              " 'прочти его мне',\n",
              " 'меня оскорбили p',\n",
              " 'было p поздно',\n",
              " 'не врите',\n",
              " 'просыпайтесь',\n",
              " 'пойдём туда',\n",
              " 'сосчитай до трёх',\n",
              " 'не забывай',\n",
              " 'дайте мне посмотреть',\n",
              " 'у меня тринадцать кошек',\n",
              " 'я ненавижу молоко',\n",
              " 'вы были p правы?',\n",
              " 'шёл p снег',\n",
              " 'том некрасив',\n",
              " 'я встал p поздно',\n",
              " 'оставь ключ',\n",
              " 'возвращайся домой',\n",
              " 'все заплатили p',\n",
              " 'я вижу дымок',\n",
              " 'я взял p книгу',\n",
              " 'я быстро говорю',\n",
              " 'я стою',\n",
              " 'сделай это сейчас том',\n",
              " 'ты выглядишь счастливой',\n",
              " 'не обращай на это внимания',\n",
              " 'не отвлекайся',\n",
              " 'факты есть факты',\n",
              " 'уходи',\n",
              " 'том нас впустил p',\n",
              " 'перестань толкаться',\n",
              " 'том заблудился?',\n",
              " 'прочитай это вслух',\n",
              " 'он задыхался p',\n",
              " 'он не может подождать?',\n",
              " 'я её нашёл p',\n",
              " 'попробуйте меня остановить',\n",
              " 'им скучно',\n",
              " 'можешь смеяться',\n",
              " 'она пустая',\n",
              " 'это кому?',\n",
              " 'я её поймал p',\n",
              " 'я не помогал p',\n",
              " 'я это прочту',\n",
              " 'я любила p бостон',\n",
              " 'я не могу устоять',\n",
              " 'мы остаёмся',\n",
              " 'он готов?',\n",
              " 'я не боюсь',\n",
              " 'стой неподвижно',\n",
              " 'я был p приглашён',\n",
              " 'оно разбито?',\n",
              " 'вы умный',\n",
              " 'я застенчивая',\n",
              " 'вы богаты',\n",
              " 'теперь посмотри это',\n",
              " 'пойдём пешком?',\n",
              " 'бросай оружие',\n",
              " 'дай посмотреть',\n",
              " 'мне нравится английский',\n",
              " 'она у нас',\n",
              " 'я благодарна',\n",
              " 'вы ненормальная?',\n",
              " 'я попытался p снова',\n",
              " 'том согласится?',\n",
              " 'я его вымыл p',\n",
              " 'том это купил?',\n",
              " 'ты мне больше не нужна',\n",
              " 'я не шучу',\n",
              " 'я встретил p друга',\n",
              " 'я путешественник',\n",
              " 'я встал p в очередь',\n",
              " 'я совсем занят',\n",
              " 'ты слаб',\n",
              " 'нам нужно поесть',\n",
              " 'видите это?',\n",
              " 'мне может понадобиться помощь',\n",
              " 'вы наивны',\n",
              " 'не будь скупым',\n",
              " 'том побрился p',\n",
              " 'я предусмотрительный',\n",
              " 'вы им пользовались?',\n",
              " 'я нормально выгляжу?',\n",
              " 'том потерялся p',\n",
              " 'том не застенчив',\n",
              " 'меня берут на работу?',\n",
              " 'вы проиграли p',\n",
              " 'она не права',\n",
              " 'верни это назад',\n",
              " 'что упущено?',\n",
              " 'я был p снисходителен',\n",
              " 'том ранен',\n",
              " 'кто это нарисовал?',\n",
              " 'это неплохо',\n",
              " 'ненавижу воскресенья',\n",
              " 'я сегодня плакала p',\n",
              " 'я был p сильным',\n",
              " 'вооружайтесь',\n",
              " 'мне просто везет',\n",
              " 'не толкай',\n",
              " 'я так боюсь',\n",
              " 'это была p ложь',\n",
              " 'мы были p бунтовщиками',\n",
              " 'не дерзи мне',\n",
              " 'это обман?',\n",
              " 'том я дома',\n",
              " 'том высказался p',\n",
              " 'я сделал p это',\n",
              " 'вы не заплатили p',\n",
              " 'уйди пожалуйста',\n",
              " 'она тебе нужна?',\n",
              " 'мы люди',\n",
              " 'полностью согласен',\n",
              " 'том с нами',\n",
              " 'полностью согласна',\n",
              " 'была p ночь',\n",
              " 'том решил p',\n",
              " 'это была p не я',\n",
              " 'не помогай тому',\n",
              " 'вернись',\n",
              " 'том такой классный',\n",
              " 'том — алкаш',\n",
              " 'вы такие плохие',\n",
              " 'помешайте суп',\n",
              " 'у меня есть пистолет',\n",
              " 'он был p выставлен на продажу',\n",
              " 'я очень больна',\n",
              " 'поезд ушёл p',\n",
              " 'допейте это',\n",
              " 'у меня есть сестраблизнец',\n",
              " 'сгодится',\n",
              " 'я снова проиграла p',\n",
              " 'я согласен с вами ребята',\n",
              " 'мне нужен воздух',\n",
              " 'у меня есть яблоко',\n",
              " 'вы выглядите счастливыми',\n",
              " 'какая у неё высота?',\n",
              " 'я часто пла́чу',\n",
              " 'это подойдёт',\n",
              " 'я заслужила p это',\n",
              " 'звони в полицию',\n",
              " 'я замешана',\n",
              " 'том был p шпионом',\n",
              " 'ты устала p',\n",
              " 'мне стало p жарко',\n",
              " 'том увидел p её',\n",
              " 'том может идти',\n",
              " 'спускайтесь сюда',\n",
              " 'это здорово',\n",
              " 'том только что ушёл p',\n",
              " 'я чувствую свой возраст',\n",
              " 'это глупо',\n",
              " 'не опаздывайте',\n",
              " 'я может быть вернусь',\n",
              " 'я здорова',\n",
              " 'два часа дня',\n",
              " 'остановите их',\n",
              " 'ты уезжаешь?',\n",
              " 'мы чуть не уехали p',\n",
              " 'принимай командование',\n",
              " 'помогло?',\n",
              " 'просто подпишитесь здесь',\n",
              " 'позвони комунибудь',\n",
              " 'я состриг чёлку',\n",
              " 'мы все повеселились p',\n",
              " 'том поедет',\n",
              " 'откройте сейф',\n",
              " 'отложи это',\n",
              " 'я потом заплачу',\n",
              " 'тому больно',\n",
              " 'было p пасмурно',\n",
              " 'мой ослик умер',\n",
              " 'я изголодалась p',\n",
              " 'я чувствую себя в безопасности',\n",
              " 'включите её',\n",
              " 'не ври мне',\n",
              " 'открой эту дверь',\n",
              " 'мы это сделаем',\n",
              " 'вам том нравится?',\n",
              " 'ты вернулся p',\n",
              " 'я съела p банан',\n",
              " 'я тебя нашёл p',\n",
              " 'покажите нам ещё',\n",
              " 'не огорчайтесь',\n",
              " 'я не поеду',\n",
              " 'я высокая',\n",
              " 'подай мне мою шляпу',\n",
              " 'я могу прийти?',\n",
              " 'мы её найдём',\n",
              " 'это шутка?',\n",
              " 'я сказал p что спою',\n",
              " 'ты счастлив?',\n",
              " 'давайте уволим тома',\n",
              " 'позвоните в полицию',\n",
              " 'я очень хотела p есть',\n",
              " 'том зол p',\n",
              " 'что я выиграл?',\n",
              " 'её лицо осветилось p',\n",
              " 'том странный',\n",
              " 'это плохо кончилось p',\n",
              " 'веди ты',\n",
              " 'ешь больше хлеба',\n",
              " 'придёшь ли p ты?',\n",
              " 'мне надо расслабиться',\n",
              " 'он получил p тройку',\n",
              " 'он мой враг',\n",
              " 'я работаю быстро',\n",
              " 'том одобрил p',\n",
              " 'ты такая ленивая',\n",
              " 'я скучала p по тому',\n",
              " 'том может измениться',\n",
              " 'я колебался p',\n",
              " 'нас никто не встретил p',\n",
              " 'мы так заняты',\n",
              " 'я сейчас пойду',\n",
              " 'это тома',\n",
              " 'можем ли p мы сделать это?',\n",
              " 'я села p',\n",
              " 'я доволен',\n",
              " 'началась p война',\n",
              " 'дай мне какуюнибудь работу',\n",
              " 'прекратите врать',\n",
              " 'видите?',\n",
              " 'я должен тебя увидеть',\n",
              " 'там закрыто',\n",
              " 'я тома встретил p',\n",
              " 'прекрати пожалуйста',\n",
              " 'том юн',\n",
              " 'он может читать',\n",
              " 'просто уходите',\n",
              " 'включи cnn',\n",
              " 'ты не занят?',\n",
              " 'пора',\n",
              " 'меня обокрали p',\n",
              " 'отойди',\n",
              " 'спасибо том',\n",
              " 'входи давай',\n",
              " 'я почувствовал p себя беспомощным',\n",
              " 'вставай том',\n",
              " 'я быстро хожу',\n",
              " 'я очень занят',\n",
              " 'я был p вам нужен',\n",
              " 'тому пришлось p побежать',\n",
              " 'том тронут',\n",
              " 'ты можешь быть уверен?',\n",
              " 'не прикасайтесь ко мне',\n",
              " 'не рискуй',\n",
              " 'жизнь хороша',\n",
              " 'том ушёл p в отставку',\n",
              " 'это так плохо?',\n",
              " 'я сделала p паузу',\n",
              " 'я никогда не видела p тома',\n",
              " 'я в банде',\n",
              " 'мне нравится ваш кот',\n",
              " 'пиво пожалуйста',\n",
              " 'всё нормально',\n",
              " 'я была p доверчива',\n",
              " 'иду',\n",
              " 'это нехорошо',\n",
              " 'теперь я уверена',\n",
              " 'надеюсь это поможет',\n",
              " 'кто они такие?',\n",
              " 'я позвоню тому',\n",
              " 'я никогда тебя не видел p',\n",
              " 'это том сделал?',\n",
              " 'позови доктора',\n",
              " 'мы умные',\n",
              " 'он пишет книги',\n",
              " 'мне надо поесть',\n",
              " 'я ранена',\n",
              " 'это неестественно',\n",
              " 'как рука?',\n",
              " 'возьмите мою',\n",
              " 'мне нужна почтовая марка',\n",
              " 'это проклятие',\n",
              " 'отвали p я сказал p',\n",
              " 'я опустошена',\n",
              " ...]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "russian_lemmas = []\n",
        "\n",
        "nlp = spacy.load(\"ru_core_news_sm\")\n",
        "\n",
        "counter = 0\n",
        "for line in russian_list:\n",
        "    counter+= 1\n",
        "    print(counter) # so I can get an estimate for how much longer it will take by watching the numbers increase\n",
        "    doc = nlp(line)\n",
        "    lemmas = [token.lemma_ if token.text != 'p' else 'p' for token in doc] #so that the past tense remains tagged\n",
        "    russian_lemmas.append(lemmas)\n",
        "\n",
        "\n",
        "print(russian_lemmas)"
      ],
      "metadata": {
        "id": "ESMNphhcI5BV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow"
      ],
      "metadata": {
        "id": "JcHpnH8OOGtV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0f7cc738-947d-4299-cf96-ca9ae4ef5cc5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.15.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (23.5.26)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.5.4)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.9.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (16.0.6)\n",
            "Requirement already satisfied: ml-dtypes~=0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.23.5)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (23.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.5.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.35.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.60.0)\n",
            "Requirement already satisfied: tensorboard<2.16,>=2.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.1)\n",
            "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.0)\n",
            "Requirement already satisfied: keras<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.42.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (1.2.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.5.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.31.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.0.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (5.3.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2023.11.17)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow) (2.1.3)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.5.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (3.2.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# this will introduce duplicates but we will delete them later\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "rus_tokenizer = Tokenizer(oov_token=\"<OOV>\") #for handeling unknown tokens\n",
        "eng_tokenizer = Tokenizer(oov_token=\"<OOV>\")\n",
        "\n",
        "\n",
        "rus_tokenizer.fit_on_texts(russian_lemmas)\n",
        "\n",
        "rus_sequences = rus_tokenizer.texts_to_sequences(russian_lemmas)\n",
        "\n",
        "rus_vocab_size = len(rus_tokenizer.word_index) + 1\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "eng_tokenizer.fit_on_texts(english_list)\n",
        "\n",
        "eng_sequences = eng_tokenizer.texts_to_sequences(english_list)\n",
        "\n",
        "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
        "\n",
        "final_data = pd.DataFrame({'rus': rus_sequences, 'eng': eng_sequences})\n",
        "\n",
        "\n",
        "# as promised, we are deleting the dups\n",
        "final_data = final_data.drop_duplicates(subset='rus', keep='first')\n",
        "\n",
        "\n",
        "# so that we can do this in a seperate runtime to preserve RAM on google collab\n",
        "import json\n",
        "\n",
        "with open('/content/rus_tokenizer.json', 'w') as file:\n",
        "    file.write(rus_tokenizer.to_json())\n",
        "\n",
        "with open('/content/eng_tokenizer.json', 'w') as file:\n",
        "    file.write(eng_tokenizer.to_json())"
      ],
      "metadata": {
        "id": "GtcOz4liMReA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "final_data['rus_lens'] = final_data['rus'].apply(lambda x: len(x))\n",
        "\n",
        "max_seq_rus = max(final_data['rus_lens'])\n",
        "print(max_seq_rus)\n",
        "\n",
        "final_data['eng_lens'] = final_data['eng'].apply(lambda x: len(x))\n",
        "\n",
        "max_seq_eng = max(final_data['eng_lens'])\n",
        "print(max_seq_rus)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y0TjOBxThIMO",
        "outputId": "ef3c599a-23f9-49e7-f953-6e63f14e4e6c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10\n",
            "10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-12-4e0660cb60be>:1: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  final_data['rus_lens'] = final_data['rus'].apply(lambda x: len(x))\n",
            "<ipython-input-12-4e0660cb60be>:6: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  final_data['eng_lens'] = final_data['eng'].apply(lambda x: len(x))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "final_data['padded_rus'] = final_data['rus'].apply(lambda x: pad_sequences([x], maxlen=max_seq_rus, padding='post', truncating='post')[0])\n",
        "final_data['padded_eng'] = final_data['eng'].apply(lambda x: pad_sequences([x], maxlen=max_seq_eng, padding='post', truncating='post')[0])\n",
        "\n"
      ],
      "metadata": {
        "id": "oVqTgBbVivXj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# one hot encode the output:\n",
        "\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "y = final_data['padded_eng'].apply(lambda x: to_categorical(x, num_classes=eng_vocab_size))\n",
        "\n"
      ],
      "metadata": {
        "id": "WsxupR6FlDQu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "y_array = np.array(y.tolist())\n",
        "print(y_array.shape)\n",
        "y_reshaped = y_array.reshape(y_array.shape[0], y_array.shape[1], -1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aLIv3SryxDQl",
        "outputId": "a4fd21fd-9ead-4cf2-e5ab-2117345ae6ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(19851, 5, 3212)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_array.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iM-DI0wqpQxO",
        "outputId": "0bee5a7a-224d-44a2-bab9-2e813e2f2927"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(19851, 5, 3212)"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EUKhb6_YfJQ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X = final_data['padded_rus']\n",
        "y = y_reshaped\n",
        "\n",
        "for x in y[:10]:\n",
        "  print(len(x[0]))\n",
        "\n",
        "temp_X, test_X, temp_y, test_y = train_test_split(X, y, test_size=0.1, random_state=24)\n",
        "\n",
        "\n",
        "train_X, val_X, train_y, val_y = train_test_split(temp_X, temp_y, test_size=0.1, random_state=24)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LLc690A7Cozq",
        "outputId": "4bf1a5c3-06d0-4c82-b439-cadaa2ceae23"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3212\n",
            "3212\n",
            "3212\n",
            "3212\n",
            "3212\n",
            "3212\n",
            "3212\n",
            "3212\n",
            "3212\n",
            "3212\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "train_X = np.array(train_X.values.tolist())\n",
        "\n",
        "train_X = train_X.astype(np.float32)\n",
        "\n",
        "test_X = np.array(test_X.values.tolist())\n",
        "\n",
        "test_X = test_X.astype(np.float32)\n",
        "\n",
        "val_X = np.array(val_X.values.tolist())\n",
        "\n",
        "val_X = val_X.astype(np.float32)\n",
        "\n"
      ],
      "metadata": {
        "id": "crn__PDXj1eK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_X.shape\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9dCJyTSbwHC9",
        "outputId": "983b2887-0aed-4a40-c1a3-699a2f3268c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(16078, 10)"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_y.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nGbihF5KwL-s",
        "outputId": "fbfb8d10-f978-44d6-c95d-91e184c73de3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(16078, 5, 3212)"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_X"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QKgC_QVxp242",
        "outputId": "a2f90d6f-4cfc-4db0-a0c3-b94bb6864fae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[  2.,  50.,   3., ...,   0.,   0.,   0.],\n",
              "       [987.,   0.,   0., ...,   0.,   0.,   0.],\n",
              "       [403.,  18., 179., ...,   0.,   0.,   0.],\n",
              "       ...,\n",
              "       [ 64.,  39.,   3., ...,   0.,   0.,   0.],\n",
              "       [195.,  16.,   5., ...,   0.,   0.,   0.],\n",
              "       [ 16., 116.,  29., ...,   0.,   0.,   0.]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OxUGuZJmp4gF",
        "outputId": "da085d46-1376-464d-edcb-986be79e801d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[[0., 0., 1., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 1., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.]],\n",
              "\n",
              "       [[0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [1., 0., 0., ..., 0., 0., 0.],\n",
              "        [1., 0., 0., ..., 0., 0., 0.],\n",
              "        [1., 0., 0., ..., 0., 0., 0.]],\n",
              "\n",
              "       [[0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [1., 0., 0., ..., 0., 0., 0.],\n",
              "        [1., 0., 0., ..., 0., 0., 0.]],\n",
              "\n",
              "       ...,\n",
              "\n",
              "       [[0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [1., 0., 0., ..., 0., 0., 0.],\n",
              "        [1., 0., 0., ..., 0., 0., 0.]],\n",
              "\n",
              "       [[0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [1., 0., 0., ..., 0., 0., 0.],\n",
              "        [1., 0., 0., ..., 0., 0., 0.]],\n",
              "\n",
              "       [[0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [1., 0., 0., ..., 0., 0., 0.],\n",
              "        [1., 0., 0., ..., 0., 0., 0.]]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "ZdpwiAaIQTfD"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1ahklxGiQTRx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# my RAM keeps running out, so this will buy me some time.\n",
        "# I will restart my runtime and pick up from here\n",
        "np.save('/content/train_X.npy', train_X)\n",
        "np.save('/content/train_y.npy', train_y)\n",
        "np.save('/content/test_X.npy', test_X)\n",
        "np.save('/content/test_y.npy', test_y)\n",
        "np.save('/content/val_X.npy', val_X)\n",
        "np.save('/content/val_y.npy', val_y)"
      ],
      "metadata": {
        "id": "dYAmN8SyPBkJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "train_X = np.load('/content/train_X.npy')\n",
        "train_y = np.load('/content/train_y.npy')\n",
        "test_X = np.load('/content/test_X.npy')\n",
        "test_y = np.load('/content/test_y.npy')\n",
        "val_X = np.load('/content/val_X.npy')\n",
        "val_y = np.load('/content/val_y.npy')"
      ],
      "metadata": {
        "id": "nBEO-8ZaQdBf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.text import tokenizer_from_json\n",
        "\n",
        "\n",
        "with open('/content/rus_tokenizer.json', 'r') as file:\n",
        "    rus_tokenizer = tokenizer_from_json(file.read())\n",
        "\n",
        "with open('/content/eng_tokenizer.json', 'r') as file:\n",
        "    eng_tokenizer = tokenizer_from_json(file.read())\n",
        "\n",
        "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
        "rus_vocab_size = len(rus_tokenizer.word_index) + 1"
      ],
      "metadata": {
        "id": "wW2Z-9gjRhn3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, RepeatVector, TimeDistributed, Dense\n",
        "from tensorflow.keras.utils import plot_model\n",
        "\n",
        "def create_model(src_vocab, tar_vocab, src_timesteps, tar_timesteps, n_units):\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(src_vocab, n_units, input_length=src_timesteps, mask_zero=True))\n",
        "    model.add(LSTM(n_units))\n",
        "    model.add(RepeatVector(tar_timesteps))\n",
        "    model.add(LSTM(n_units, return_sequences=True))\n",
        "    model.add(TimeDistributed(Dense(tar_vocab, activation='softmax')))\n",
        "    model.compile(optimizer='adam', loss='categorical_crossentropy')\n",
        "    return model\n",
        "\n",
        "\n",
        "\n",
        "units = [128, 256]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Om8I3UAR0L1b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(rus_vocab_size)\n",
        "print(eng_vocab_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y869PN_73gQ8",
        "outputId": "b874d9ec-2282-42c6-c3f8-973ea8e163c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5660\n",
            "3212\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.callbacks import ModelCheckpoint\n",
        "best_model = None\n",
        "best_val_loss = float('inf')\n",
        "for unit in units:\n",
        "  model = create_model(rus_vocab_size, eng_vocab_size, 10, 5, unit)\n",
        "  print(model.summary())\n",
        "  filename = f'/content/best_model_{unit}.h5'\n",
        "  checkpoint = ModelCheckpoint(filename, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
        "  model.fit(train_X, train_y, epochs=30, batch_size=100, validation_data=(val_X, val_y), callbacks=[checkpoint], verbose=1)\n",
        "  eval_loss = model.evaluate(test_X, test_y, verbose=1)\n",
        "\n",
        "  print(f\"testing loss for unit {unit}: {eval_loss}\")\n",
        "\n",
        "  if eval_loss < best_val_loss:\n",
        "      best_val_loss = eval_loss\n",
        "      best_model = model\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hJCQN53tNPqW",
        "outputId": "c1e56129-095a-4367-c5da-cdf1bf5c54e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_1 (Embedding)     (None, 10, 128)           724480    \n",
            "                                                                 \n",
            " lstm_2 (LSTM)               (None, 128)               131584    \n",
            "                                                                 \n",
            " repeat_vector_1 (RepeatVec  (None, 5, 128)            0         \n",
            " tor)                                                            \n",
            "                                                                 \n",
            " lstm_3 (LSTM)               (None, 5, 128)            131584    \n",
            "                                                                 \n",
            " time_distributed_1 (TimeDi  (None, 5, 3212)           414348    \n",
            " stributed)                                                      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1401996 (5.35 MB)\n",
            "Trainable params: 1401996 (5.35 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/30\n",
            "161/161 [==============================] - ETA: 0s - loss: 4.9311\n",
            "Epoch 1: val_loss improved from inf to 4.21917, saving model to /content/best_model_128.h5\n",
            "161/161 [==============================] - 43s 217ms/step - loss: 4.9311 - val_loss: 4.2192\n",
            "Epoch 2/30\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "161/161 [==============================] - ETA: 0s - loss: 4.0593\n",
            "Epoch 2: val_loss improved from 4.21917 to 3.97934, saving model to /content/best_model_128.h5\n",
            "161/161 [==============================] - 35s 219ms/step - loss: 4.0593 - val_loss: 3.9793\n",
            "Epoch 3/30\n",
            "161/161 [==============================] - ETA: 0s - loss: 3.8375\n",
            "Epoch 3: val_loss improved from 3.97934 to 3.80126, saving model to /content/best_model_128.h5\n",
            "161/161 [==============================] - 28s 173ms/step - loss: 3.8375 - val_loss: 3.8013\n",
            "Epoch 4/30\n",
            "161/161 [==============================] - ETA: 0s - loss: 3.6910\n",
            "Epoch 4: val_loss improved from 3.80126 to 3.70300, saving model to /content/best_model_128.h5\n",
            "161/161 [==============================] - 28s 176ms/step - loss: 3.6910 - val_loss: 3.7030\n",
            "Epoch 5/30\n",
            "161/161 [==============================] - ETA: 0s - loss: 3.5932\n",
            "Epoch 5: val_loss improved from 3.70300 to 3.61697, saving model to /content/best_model_128.h5\n",
            "161/161 [==============================] - 32s 196ms/step - loss: 3.5932 - val_loss: 3.6170\n",
            "Epoch 6/30\n",
            "161/161 [==============================] - ETA: 0s - loss: 3.4689\n",
            "Epoch 6: val_loss improved from 3.61697 to 3.43295, saving model to /content/best_model_128.h5\n",
            "161/161 [==============================] - 28s 173ms/step - loss: 3.4689 - val_loss: 3.4329\n",
            "Epoch 7/30\n",
            "161/161 [==============================] - ETA: 0s - loss: 3.2741\n",
            "Epoch 7: val_loss improved from 3.43295 to 3.26979, saving model to /content/best_model_128.h5\n",
            "161/161 [==============================] - 28s 173ms/step - loss: 3.2741 - val_loss: 3.2698\n",
            "Epoch 8/30\n",
            "161/161 [==============================] - ETA: 0s - loss: 3.1104\n",
            "Epoch 8: val_loss improved from 3.26979 to 3.14416, saving model to /content/best_model_128.h5\n",
            "161/161 [==============================] - 28s 176ms/step - loss: 3.1104 - val_loss: 3.1442\n",
            "Epoch 9/30\n",
            "161/161 [==============================] - ETA: 0s - loss: 2.9621\n",
            "Epoch 9: val_loss improved from 3.14416 to 3.03104, saving model to /content/best_model_128.h5\n",
            "161/161 [==============================] - 28s 173ms/step - loss: 2.9621 - val_loss: 3.0310\n",
            "Epoch 10/30\n",
            "161/161 [==============================] - ETA: 0s - loss: 2.8176\n",
            "Epoch 10: val_loss improved from 3.03104 to 2.91406, saving model to /content/best_model_128.h5\n",
            "161/161 [==============================] - 28s 174ms/step - loss: 2.8176 - val_loss: 2.9141\n",
            "Epoch 11/30\n",
            "161/161 [==============================] - ETA: 0s - loss: 2.6780\n",
            "Epoch 11: val_loss improved from 2.91406 to 2.78544, saving model to /content/best_model_128.h5\n",
            "161/161 [==============================] - 29s 178ms/step - loss: 2.6780 - val_loss: 2.7854\n",
            "Epoch 12/30\n",
            "161/161 [==============================] - ETA: 0s - loss: 2.5419\n",
            "Epoch 12: val_loss improved from 2.78544 to 2.68612, saving model to /content/best_model_128.h5\n",
            "161/161 [==============================] - 31s 192ms/step - loss: 2.5419 - val_loss: 2.6861\n",
            "Epoch 13/30\n",
            "161/161 [==============================] - ETA: 0s - loss: 2.4200\n",
            "Epoch 13: val_loss improved from 2.68612 to 2.59549, saving model to /content/best_model_128.h5\n",
            "161/161 [==============================] - 29s 180ms/step - loss: 2.4200 - val_loss: 2.5955\n",
            "Epoch 14/30\n",
            "161/161 [==============================] - ETA: 0s - loss: 2.3092\n",
            "Epoch 14: val_loss improved from 2.59549 to 2.51769, saving model to /content/best_model_128.h5\n",
            "161/161 [==============================] - 27s 167ms/step - loss: 2.3092 - val_loss: 2.5177\n",
            "Epoch 15/30\n",
            "161/161 [==============================] - ETA: 0s - loss: 2.2054\n",
            "Epoch 15: val_loss improved from 2.51769 to 2.43873, saving model to /content/best_model_128.h5\n",
            "161/161 [==============================] - 27s 170ms/step - loss: 2.2054 - val_loss: 2.4387\n",
            "Epoch 16/30\n",
            "161/161 [==============================] - ETA: 0s - loss: 2.1070\n",
            "Epoch 16: val_loss improved from 2.43873 to 2.37349, saving model to /content/best_model_128.h5\n",
            "161/161 [==============================] - 27s 166ms/step - loss: 2.1070 - val_loss: 2.3735\n",
            "Epoch 17/30\n",
            "161/161 [==============================] - ETA: 0s - loss: 2.0156\n",
            "Epoch 17: val_loss improved from 2.37349 to 2.31572, saving model to /content/best_model_128.h5\n",
            "161/161 [==============================] - 27s 169ms/step - loss: 2.0156 - val_loss: 2.3157\n",
            "Epoch 18/30\n",
            "161/161 [==============================] - ETA: 0s - loss: 1.9268\n",
            "Epoch 18: val_loss improved from 2.31572 to 2.26939, saving model to /content/best_model_128.h5\n",
            "161/161 [==============================] - 27s 166ms/step - loss: 1.9268 - val_loss: 2.2694\n",
            "Epoch 19/30\n",
            "161/161 [==============================] - ETA: 0s - loss: 1.8457\n",
            "Epoch 19: val_loss improved from 2.26939 to 2.20963, saving model to /content/best_model_128.h5\n",
            "161/161 [==============================] - 29s 179ms/step - loss: 1.8457 - val_loss: 2.2096\n",
            "Epoch 20/30\n",
            "161/161 [==============================] - ETA: 0s - loss: 1.7649\n",
            "Epoch 20: val_loss improved from 2.20963 to 2.17149, saving model to /content/best_model_128.h5\n",
            "161/161 [==============================] - 34s 213ms/step - loss: 1.7649 - val_loss: 2.1715\n",
            "Epoch 21/30\n",
            "161/161 [==============================] - ETA: 0s - loss: 1.6862\n",
            "Epoch 21: val_loss improved from 2.17149 to 2.11743, saving model to /content/best_model_128.h5\n",
            "161/161 [==============================] - 28s 173ms/step - loss: 1.6862 - val_loss: 2.1174\n",
            "Epoch 22/30\n",
            "161/161 [==============================] - ETA: 0s - loss: 1.6104\n",
            "Epoch 22: val_loss improved from 2.11743 to 2.07183, saving model to /content/best_model_128.h5\n",
            "161/161 [==============================] - 28s 172ms/step - loss: 1.6104 - val_loss: 2.0718\n",
            "Epoch 23/30\n",
            "161/161 [==============================] - ETA: 0s - loss: 1.5383\n",
            "Epoch 23: val_loss improved from 2.07183 to 2.03266, saving model to /content/best_model_128.h5\n",
            "161/161 [==============================] - 29s 177ms/step - loss: 1.5383 - val_loss: 2.0327\n",
            "Epoch 24/30\n",
            "161/161 [==============================] - ETA: 0s - loss: 1.4665\n",
            "Epoch 24: val_loss improved from 2.03266 to 2.00058, saving model to /content/best_model_128.h5\n",
            "161/161 [==============================] - 37s 232ms/step - loss: 1.4665 - val_loss: 2.0006\n",
            "Epoch 25/30\n",
            "161/161 [==============================] - ETA: 0s - loss: 1.3963\n",
            "Epoch 25: val_loss improved from 2.00058 to 1.96450, saving model to /content/best_model_128.h5\n",
            "161/161 [==============================] - 34s 208ms/step - loss: 1.3963 - val_loss: 1.9645\n",
            "Epoch 26/30\n",
            "161/161 [==============================] - ETA: 0s - loss: 1.3295\n",
            "Epoch 26: val_loss improved from 1.96450 to 1.93263, saving model to /content/best_model_128.h5\n",
            "161/161 [==============================] - 32s 199ms/step - loss: 1.3295 - val_loss: 1.9326\n",
            "Epoch 27/30\n",
            "161/161 [==============================] - ETA: 0s - loss: 1.2598\n",
            "Epoch 27: val_loss improved from 1.93263 to 1.90479, saving model to /content/best_model_128.h5\n",
            "161/161 [==============================] - 29s 177ms/step - loss: 1.2598 - val_loss: 1.9048\n",
            "Epoch 28/30\n",
            "161/161 [==============================] - ETA: 0s - loss: 1.1932\n",
            "Epoch 28: val_loss improved from 1.90479 to 1.86790, saving model to /content/best_model_128.h5\n",
            "161/161 [==============================] - 29s 180ms/step - loss: 1.1932 - val_loss: 1.8679\n",
            "Epoch 29/30\n",
            "161/161 [==============================] - ETA: 0s - loss: 1.1306\n",
            "Epoch 29: val_loss improved from 1.86790 to 1.84418, saving model to /content/best_model_128.h5\n",
            "161/161 [==============================] - 28s 174ms/step - loss: 1.1306 - val_loss: 1.8442\n",
            "Epoch 30/30\n",
            "161/161 [==============================] - ETA: 0s - loss: 1.0683\n",
            "Epoch 30: val_loss improved from 1.84418 to 1.81930, saving model to /content/best_model_128.h5\n",
            "161/161 [==============================] - 28s 177ms/step - loss: 1.0683 - val_loss: 1.8193\n",
            "63/63 [==============================] - 1s 20ms/step - loss: 1.8588\n",
            "testing loss for unit 128: 1.8587579727172852\n",
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_2 (Embedding)     (None, 10, 256)           1448960   \n",
            "                                                                 \n",
            " lstm_4 (LSTM)               (None, 256)               525312    \n",
            "                                                                 \n",
            " repeat_vector_2 (RepeatVec  (None, 5, 256)            0         \n",
            " tor)                                                            \n",
            "                                                                 \n",
            " lstm_5 (LSTM)               (None, 5, 256)            525312    \n",
            "                                                                 \n",
            " time_distributed_2 (TimeDi  (None, 5, 3212)           825484    \n",
            " stributed)                                                      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 3325068 (12.68 MB)\n",
            "Trainable params: 3325068 (12.68 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/30\n",
            "161/161 [==============================] - ETA: 0s - loss: 4.4793\n",
            "Epoch 1: val_loss improved from inf to 3.65278, saving model to /content/best_model_256.h5\n",
            "161/161 [==============================] - 66s 361ms/step - loss: 4.4793 - val_loss: 3.6528\n",
            "Epoch 2/30\n",
            "161/161 [==============================] - ETA: 0s - loss: 3.4737\n",
            "Epoch 2: val_loss improved from 3.65278 to 3.39172, saving model to /content/best_model_256.h5\n",
            "161/161 [==============================] - 56s 345ms/step - loss: 3.4737 - val_loss: 3.3917\n",
            "Epoch 3/30\n",
            "161/161 [==============================] - ETA: 0s - loss: 3.2468\n",
            "Epoch 3: val_loss improved from 3.39172 to 3.19676, saving model to /content/best_model_256.h5\n",
            "161/161 [==============================] - 55s 344ms/step - loss: 3.2468 - val_loss: 3.1968\n",
            "Epoch 4/30\n",
            "161/161 [==============================] - ETA: 0s - loss: 3.0042\n",
            "Epoch 4: val_loss improved from 3.19676 to 2.98858, saving model to /content/best_model_256.h5\n",
            "161/161 [==============================] - 55s 344ms/step - loss: 3.0042 - val_loss: 2.9886\n",
            "Epoch 5/30\n",
            "161/161 [==============================] - ETA: 0s - loss: 2.7697\n",
            "Epoch 5: val_loss improved from 2.98858 to 2.79306, saving model to /content/best_model_256.h5\n",
            "161/161 [==============================] - 61s 381ms/step - loss: 2.7697 - val_loss: 2.7931\n",
            "Epoch 6/30\n",
            "161/161 [==============================] - ETA: 0s - loss: 2.5378\n",
            "Epoch 6: val_loss improved from 2.79306 to 2.61083, saving model to /content/best_model_256.h5\n",
            "161/161 [==============================] - 56s 347ms/step - loss: 2.5378 - val_loss: 2.6108\n",
            "Epoch 7/30\n",
            "161/161 [==============================] - ETA: 0s - loss: 2.3413\n",
            "Epoch 7: val_loss improved from 2.61083 to 2.49422, saving model to /content/best_model_256.h5\n",
            "161/161 [==============================] - 56s 348ms/step - loss: 2.3413 - val_loss: 2.4942\n",
            "Epoch 8/30\n",
            "161/161 [==============================] - ETA: 0s - loss: 2.1741\n",
            "Epoch 8: val_loss improved from 2.49422 to 2.38083, saving model to /content/best_model_256.h5\n",
            "161/161 [==============================] - 59s 365ms/step - loss: 2.1741 - val_loss: 2.3808\n",
            "Epoch 9/30\n",
            "161/161 [==============================] - ETA: 0s - loss: 2.0254\n",
            "Epoch 9: val_loss improved from 2.38083 to 2.28235, saving model to /content/best_model_256.h5\n",
            "161/161 [==============================] - 57s 354ms/step - loss: 2.0254 - val_loss: 2.2823\n",
            "Epoch 10/30\n",
            "161/161 [==============================] - ETA: 0s - loss: 1.8927\n",
            "Epoch 10: val_loss improved from 2.28235 to 2.19667, saving model to /content/best_model_256.h5\n",
            "161/161 [==============================] - 55s 343ms/step - loss: 1.8927 - val_loss: 2.1967\n",
            "Epoch 11/30\n",
            "161/161 [==============================] - ETA: 0s - loss: 1.7670\n",
            "Epoch 11: val_loss improved from 2.19667 to 2.12439, saving model to /content/best_model_256.h5\n",
            "161/161 [==============================] - 58s 359ms/step - loss: 1.7670 - val_loss: 2.1244\n",
            "Epoch 12/30\n",
            "161/161 [==============================] - ETA: 0s - loss: 1.6502\n",
            "Epoch 12: val_loss improved from 2.12439 to 2.06755, saving model to /content/best_model_256.h5\n",
            "161/161 [==============================] - 57s 351ms/step - loss: 1.6502 - val_loss: 2.0675\n",
            "Epoch 13/30\n",
            "161/161 [==============================] - ETA: 0s - loss: 1.5365\n",
            "Epoch 13: val_loss improved from 2.06755 to 2.00654, saving model to /content/best_model_256.h5\n",
            "161/161 [==============================] - 53s 330ms/step - loss: 1.5365 - val_loss: 2.0065\n",
            "Epoch 14/30\n",
            "161/161 [==============================] - ETA: 0s - loss: 1.4311\n",
            "Epoch 14: val_loss improved from 2.00654 to 1.94317, saving model to /content/best_model_256.h5\n",
            "161/161 [==============================] - 54s 338ms/step - loss: 1.4311 - val_loss: 1.9432\n",
            "Epoch 15/30\n",
            "161/161 [==============================] - ETA: 0s - loss: 1.3294\n",
            "Epoch 15: val_loss improved from 1.94317 to 1.88352, saving model to /content/best_model_256.h5\n",
            "161/161 [==============================] - 57s 354ms/step - loss: 1.3294 - val_loss: 1.8835\n",
            "Epoch 16/30\n",
            "161/161 [==============================] - ETA: 0s - loss: 1.2274\n",
            "Epoch 16: val_loss improved from 1.88352 to 1.83824, saving model to /content/best_model_256.h5\n",
            "161/161 [==============================] - 56s 345ms/step - loss: 1.2274 - val_loss: 1.8382\n",
            "Epoch 17/30\n",
            "161/161 [==============================] - ETA: 0s - loss: 1.1289\n",
            "Epoch 17: val_loss improved from 1.83824 to 1.80229, saving model to /content/best_model_256.h5\n",
            "161/161 [==============================] - 56s 350ms/step - loss: 1.1289 - val_loss: 1.8023\n",
            "Epoch 18/30\n",
            "161/161 [==============================] - ETA: 0s - loss: 1.0396\n",
            "Epoch 18: val_loss improved from 1.80229 to 1.75566, saving model to /content/best_model_256.h5\n",
            "161/161 [==============================] - 57s 357ms/step - loss: 1.0396 - val_loss: 1.7557\n",
            "Epoch 19/30\n",
            "161/161 [==============================] - ETA: 0s - loss: 0.9516\n",
            "Epoch 19: val_loss improved from 1.75566 to 1.73162, saving model to /content/best_model_256.h5\n",
            "161/161 [==============================] - 57s 352ms/step - loss: 0.9516 - val_loss: 1.7316\n",
            "Epoch 20/30\n",
            "161/161 [==============================] - ETA: 0s - loss: 0.8685\n",
            "Epoch 20: val_loss improved from 1.73162 to 1.69169, saving model to /content/best_model_256.h5\n",
            "161/161 [==============================] - 57s 352ms/step - loss: 0.8685 - val_loss: 1.6917\n",
            "Epoch 21/30\n",
            "161/161 [==============================] - ETA: 0s - loss: 0.7918\n",
            "Epoch 21: val_loss improved from 1.69169 to 1.66761, saving model to /content/best_model_256.h5\n",
            "161/161 [==============================] - 58s 360ms/step - loss: 0.7918 - val_loss: 1.6676\n",
            "Epoch 22/30\n",
            "161/161 [==============================] - ETA: 0s - loss: 0.7170\n",
            "Epoch 22: val_loss improved from 1.66761 to 1.63492, saving model to /content/best_model_256.h5\n",
            "161/161 [==============================] - 57s 355ms/step - loss: 0.7170 - val_loss: 1.6349\n",
            "Epoch 23/30\n",
            "161/161 [==============================] - ETA: 0s - loss: 0.6466\n",
            "Epoch 23: val_loss improved from 1.63492 to 1.60905, saving model to /content/best_model_256.h5\n",
            "161/161 [==============================] - 56s 346ms/step - loss: 0.6466 - val_loss: 1.6091\n",
            "Epoch 24/30\n",
            "161/161 [==============================] - ETA: 0s - loss: 0.5852\n",
            "Epoch 24: val_loss improved from 1.60905 to 1.59645, saving model to /content/best_model_256.h5\n",
            "161/161 [==============================] - 57s 357ms/step - loss: 0.5852 - val_loss: 1.5965\n",
            "Epoch 25/30\n",
            "161/161 [==============================] - ETA: 0s - loss: 0.5247\n",
            "Epoch 25: val_loss improved from 1.59645 to 1.57647, saving model to /content/best_model_256.h5\n",
            "161/161 [==============================] - 57s 351ms/step - loss: 0.5247 - val_loss: 1.5765\n",
            "Epoch 26/30\n",
            "161/161 [==============================] - ETA: 0s - loss: 0.4705\n",
            "Epoch 26: val_loss improved from 1.57647 to 1.56383, saving model to /content/best_model_256.h5\n",
            "161/161 [==============================] - 57s 352ms/step - loss: 0.4705 - val_loss: 1.5638\n",
            "Epoch 27/30\n",
            "161/161 [==============================] - ETA: 0s - loss: 0.4208\n",
            "Epoch 27: val_loss improved from 1.56383 to 1.55564, saving model to /content/best_model_256.h5\n",
            "161/161 [==============================] - 56s 350ms/step - loss: 0.4208 - val_loss: 1.5556\n",
            "Epoch 28/30\n",
            "161/161 [==============================] - ETA: 0s - loss: 0.3772\n",
            "Epoch 28: val_loss improved from 1.55564 to 1.54575, saving model to /content/best_model_256.h5\n",
            "161/161 [==============================] - 56s 347ms/step - loss: 0.3772 - val_loss: 1.5457\n",
            "Epoch 29/30\n",
            "161/161 [==============================] - ETA: 0s - loss: 0.3377\n",
            "Epoch 29: val_loss improved from 1.54575 to 1.54256, saving model to /content/best_model_256.h5\n",
            "161/161 [==============================] - 56s 346ms/step - loss: 0.3377 - val_loss: 1.5426\n",
            "Epoch 30/30\n",
            "161/161 [==============================] - ETA: 0s - loss: 0.3016\n",
            "Epoch 30: val_loss improved from 1.54256 to 1.53664, saving model to /content/best_model_256.h5\n",
            "161/161 [==============================] - 55s 345ms/step - loss: 0.3016 - val_loss: 1.5366\n",
            "63/63 [==============================] - 2s 36ms/step - loss: 1.5972\n",
            "testing loss for unit 256: 1.5971723794937134\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NlixW2SLiHhJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# because we got the best score with the larger model of the 2, I will try one more training, this time with 400 units. Anything over this would probably crash my RAM\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "big_model = create_model(rus_vocab_size, eng_vocab_size, 10, 5, 400)\n",
        "print(big_model.summary())\n",
        "filename = f'/content/best_model_unit_400.h5'\n",
        "checkpoint = ModelCheckpoint(filename, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
        "big_model.fit(train_X, train_y, epochs=30, batch_size=100, validation_data=(val_X, val_y), callbacks=[checkpoint], verbose=1)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "44gg9yfjaOQD",
        "outputId": "320c9771-12e0-4df3-d120-bd2be5a8898f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_1 (Embedding)     (None, 10, 400)           2264000   \n",
            "                                                                 \n",
            " lstm_2 (LSTM)               (None, 400)               1281600   \n",
            "                                                                 \n",
            " repeat_vector_1 (RepeatVec  (None, 5, 400)            0         \n",
            " tor)                                                            \n",
            "                                                                 \n",
            " lstm_3 (LSTM)               (None, 5, 400)            1281600   \n",
            "                                                                 \n",
            " time_distributed_1 (TimeDi  (None, 5, 3212)           1288012   \n",
            " stributed)                                                      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 6115212 (23.33 MB)\n",
            "Trainable params: 6115212 (23.33 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/30\n",
            "161/161 [==============================] - ETA: 0s - loss: 4.1780\n",
            "Epoch 1: val_loss improved from inf to 3.46015, saving model to /content/best_model_unit_400.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r161/161 [==============================] - 98s 564ms/step - loss: 4.1780 - val_loss: 3.4602\n",
            "Epoch 2/30\n",
            "161/161 [==============================] - ETA: 0s - loss: 3.2974\n",
            "Epoch 2: val_loss improved from 3.46015 to 3.20060, saving model to /content/best_model_unit_400.h5\n",
            "161/161 [==============================] - 87s 542ms/step - loss: 3.2974 - val_loss: 3.2006\n",
            "Epoch 3/30\n",
            "161/161 [==============================] - ETA: 0s - loss: 2.9519\n",
            "Epoch 3: val_loss improved from 3.20060 to 2.86406, saving model to /content/best_model_unit_400.h5\n",
            "161/161 [==============================] - 86s 534ms/step - loss: 2.9519 - val_loss: 2.8641\n",
            "Epoch 4/30\n",
            "161/161 [==============================] - ETA: 0s - loss: 2.6107\n",
            "Epoch 4: val_loss improved from 2.86406 to 2.62482, saving model to /content/best_model_unit_400.h5\n",
            "161/161 [==============================] - 86s 536ms/step - loss: 2.6107 - val_loss: 2.6248\n",
            "Epoch 5/30\n",
            "161/161 [==============================] - ETA: 0s - loss: 2.3364\n",
            "Epoch 5: val_loss improved from 2.62482 to 2.43607, saving model to /content/best_model_unit_400.h5\n",
            "161/161 [==============================] - 85s 530ms/step - loss: 2.3364 - val_loss: 2.4361\n",
            "Epoch 6/30\n",
            "161/161 [==============================] - ETA: 0s - loss: 2.1009\n",
            "Epoch 6: val_loss improved from 2.43607 to 2.27032, saving model to /content/best_model_unit_400.h5\n",
            "161/161 [==============================] - 87s 544ms/step - loss: 2.1009 - val_loss: 2.2703\n",
            "Epoch 7/30\n",
            "161/161 [==============================] - ETA: 0s - loss: 1.9018\n",
            "Epoch 7: val_loss improved from 2.27032 to 2.15575, saving model to /content/best_model_unit_400.h5\n",
            "161/161 [==============================] - 86s 531ms/step - loss: 1.9018 - val_loss: 2.1557\n",
            "Epoch 8/30\n",
            "161/161 [==============================] - ETA: 0s - loss: 1.7297\n",
            "Epoch 8: val_loss improved from 2.15575 to 2.05835, saving model to /content/best_model_unit_400.h5\n",
            "161/161 [==============================] - 86s 533ms/step - loss: 1.7297 - val_loss: 2.0584\n",
            "Epoch 9/30\n",
            "161/161 [==============================] - ETA: 0s - loss: 1.5669\n",
            "Epoch 9: val_loss improved from 2.05835 to 1.96809, saving model to /content/best_model_unit_400.h5\n",
            "161/161 [==============================] - 84s 520ms/step - loss: 1.5669 - val_loss: 1.9681\n",
            "Epoch 10/30\n",
            "161/161 [==============================] - ETA: 0s - loss: 1.4165\n",
            "Epoch 10: val_loss improved from 1.96809 to 1.88861, saving model to /content/best_model_unit_400.h5\n",
            "161/161 [==============================] - 84s 520ms/step - loss: 1.4165 - val_loss: 1.8886\n",
            "Epoch 11/30\n",
            "161/161 [==============================] - ETA: 0s - loss: 1.2732\n",
            "Epoch 11: val_loss improved from 1.88861 to 1.81589, saving model to /content/best_model_unit_400.h5\n",
            "161/161 [==============================] - 84s 521ms/step - loss: 1.2732 - val_loss: 1.8159\n",
            "Epoch 12/30\n",
            "161/161 [==============================] - ETA: 0s - loss: 1.1437\n",
            "Epoch 12: val_loss improved from 1.81589 to 1.75792, saving model to /content/best_model_unit_400.h5\n",
            "161/161 [==============================] - 98s 607ms/step - loss: 1.1437 - val_loss: 1.7579\n",
            "Epoch 13/30\n",
            "161/161 [==============================] - ETA: 0s - loss: 1.0135\n",
            "Epoch 13: val_loss improved from 1.75792 to 1.69798, saving model to /content/best_model_unit_400.h5\n",
            "161/161 [==============================] - 91s 565ms/step - loss: 1.0135 - val_loss: 1.6980\n",
            "Epoch 14/30\n",
            "161/161 [==============================] - ETA: 0s - loss: 0.8943\n",
            "Epoch 14: val_loss improved from 1.69798 to 1.64202, saving model to /content/best_model_unit_400.h5\n",
            "161/161 [==============================] - 84s 523ms/step - loss: 0.8943 - val_loss: 1.6420\n",
            "Epoch 15/30\n",
            "161/161 [==============================] - ETA: 0s - loss: 0.7875\n",
            "Epoch 15: val_loss improved from 1.64202 to 1.60689, saving model to /content/best_model_unit_400.h5\n",
            "161/161 [==============================] - 85s 529ms/step - loss: 0.7875 - val_loss: 1.6069\n",
            "Epoch 16/30\n",
            "161/161 [==============================] - ETA: 0s - loss: 0.6875\n",
            "Epoch 16: val_loss improved from 1.60689 to 1.56590, saving model to /content/best_model_unit_400.h5\n",
            "161/161 [==============================] - 83s 518ms/step - loss: 0.6875 - val_loss: 1.5659\n",
            "Epoch 17/30\n",
            "161/161 [==============================] - ETA: 0s - loss: 0.5975\n",
            "Epoch 17: val_loss improved from 1.56590 to 1.54251, saving model to /content/best_model_unit_400.h5\n",
            "161/161 [==============================] - 84s 521ms/step - loss: 0.5975 - val_loss: 1.5425\n",
            "Epoch 18/30\n",
            "161/161 [==============================] - ETA: 0s - loss: 0.5130\n",
            "Epoch 18: val_loss improved from 1.54251 to 1.50822, saving model to /content/best_model_unit_400.h5\n",
            "161/161 [==============================] - 85s 525ms/step - loss: 0.5130 - val_loss: 1.5082\n",
            "Epoch 19/30\n",
            "161/161 [==============================] - ETA: 0s - loss: 0.4408\n",
            "Epoch 19: val_loss improved from 1.50822 to 1.48828, saving model to /content/best_model_unit_400.h5\n",
            "161/161 [==============================] - 86s 534ms/step - loss: 0.4408 - val_loss: 1.4883\n",
            "Epoch 20/30\n",
            "161/161 [==============================] - ETA: 0s - loss: 0.3764\n",
            "Epoch 20: val_loss improved from 1.48828 to 1.47561, saving model to /content/best_model_unit_400.h5\n",
            "161/161 [==============================] - 84s 521ms/step - loss: 0.3764 - val_loss: 1.4756\n",
            "Epoch 21/30\n",
            "161/161 [==============================] - ETA: 0s - loss: 0.3221\n",
            "Epoch 21: val_loss improved from 1.47561 to 1.46757, saving model to /content/best_model_unit_400.h5\n",
            "161/161 [==============================] - 89s 551ms/step - loss: 0.3221 - val_loss: 1.4676\n",
            "Epoch 22/30\n",
            "161/161 [==============================] - ETA: 0s - loss: 0.2770\n",
            "Epoch 22: val_loss improved from 1.46757 to 1.46434, saving model to /content/best_model_unit_400.h5\n",
            "161/161 [==============================] - 88s 548ms/step - loss: 0.2770 - val_loss: 1.4643\n",
            "Epoch 23/30\n",
            "161/161 [==============================] - ETA: 0s - loss: 0.2361\n",
            "Epoch 23: val_loss improved from 1.46434 to 1.45596, saving model to /content/best_model_unit_400.h5\n",
            "161/161 [==============================] - 89s 554ms/step - loss: 0.2361 - val_loss: 1.4560\n",
            "Epoch 24/30\n",
            "161/161 [==============================] - ETA: 0s - loss: 0.2005\n",
            "Epoch 24: val_loss improved from 1.45596 to 1.44778, saving model to /content/best_model_unit_400.h5\n",
            "161/161 [==============================] - 82s 508ms/step - loss: 0.2005 - val_loss: 1.4478\n",
            "Epoch 25/30\n",
            "161/161 [==============================] - ETA: 0s - loss: 0.1697\n",
            "Epoch 25: val_loss did not improve from 1.44778\n",
            "161/161 [==============================] - 83s 514ms/step - loss: 0.1697 - val_loss: 1.4636\n",
            "Epoch 26/30\n",
            "161/161 [==============================] - ETA: 0s - loss: 0.1430\n",
            "Epoch 26: val_loss did not improve from 1.44778\n",
            "161/161 [==============================] - 84s 522ms/step - loss: 0.1430 - val_loss: 1.4547\n",
            "Epoch 27/30\n",
            "161/161 [==============================] - ETA: 0s - loss: 0.1219\n",
            "Epoch 27: val_loss did not improve from 1.44778\n",
            "161/161 [==============================] - 83s 515ms/step - loss: 0.1219 - val_loss: 1.4671\n",
            "Epoch 28/30\n",
            "161/161 [==============================] - ETA: 0s - loss: 0.1032\n",
            "Epoch 28: val_loss did not improve from 1.44778\n",
            "161/161 [==============================] - 83s 518ms/step - loss: 0.1032 - val_loss: 1.4675\n",
            "Epoch 29/30\n",
            "161/161 [==============================] - ETA: 0s - loss: 0.0870\n",
            "Epoch 29: val_loss did not improve from 1.44778\n",
            "161/161 [==============================] - 85s 531ms/step - loss: 0.0870 - val_loss: 1.4744\n",
            "Epoch 30/30\n",
            "161/161 [==============================] - ETA: 0s - loss: 0.0744\n",
            "Epoch 30: val_loss did not improve from 1.44778\n",
            "161/161 [==============================] - 83s 517ms/step - loss: 0.0744 - val_loss: 1.4811\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x78664dc2d120>"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "model_path = '/content/best_model_unit_400.h5'\n",
        "big_model = load_model(model_path)"
      ],
      "metadata": {
        "id": "9VDivsozMg64"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "big_model.evaluate(test_X, test_y, verbose=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rvZUhmAnszVC",
        "outputId": "0ae3d473-f123-43b5-f462-2ae120fa1b4d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "63/63 [==============================] - 4s 60ms/step - loss: 1.5248\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.5248371362686157"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "the best model seems to be the one with 400 units"
      ],
      "metadata": {
        "id": "r9CROgVxvSPg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "translation = big_model.predict(test_X, verbose=1)\n",
        "print(translation)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iR02bSL-06tH",
        "outputId": "6eda1e43-4ca7-4cd0-f061-9f199556cfd4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "63/63 [==============================] - 11s 115ms/step\n",
            "[[[4.30398011e-08 2.46817584e-12 9.94995415e-01 ... 5.96067642e-19\n",
            "   1.66117185e-12 1.04383059e-19]\n",
            "  [4.04719998e-07 2.07976880e-09 2.97707098e-04 ... 2.41531296e-13\n",
            "   1.60793001e-09 1.03913080e-12]\n",
            "  [1.49138284e-08 7.07315526e-11 4.86425888e-05 ... 2.20504343e-14\n",
            "   5.29568958e-11 2.50583889e-14]\n",
            "  [3.94827127e-03 7.95358162e-08 2.04193304e-04 ... 2.11184672e-12\n",
            "   9.50405123e-08 2.14403287e-11]\n",
            "  [9.99080777e-01 6.99312136e-11 1.09625137e-06 ... 5.84129643e-16\n",
            "   9.42962444e-11 9.92301385e-15]]\n",
            "\n",
            " [[1.92966194e-08 1.16507791e-12 9.98167038e-01 ... 4.48111461e-20\n",
            "   7.69835964e-13 5.26046157e-20]\n",
            "  [6.64056259e-08 5.68297735e-11 2.37636996e-05 ... 1.62281401e-16\n",
            "   3.97515215e-11 2.26176982e-15]\n",
            "  [8.79313757e-06 2.80010326e-09 8.57503247e-03 ... 1.56708549e-13\n",
            "   1.90689486e-09 1.05356236e-11]\n",
            "  [1.89315025e-02 4.59230414e-08 1.90693231e-06 ... 4.99096814e-12\n",
            "   4.22111768e-08 1.13668308e-09]\n",
            "  [9.99911606e-01 2.86389901e-12 5.16906411e-08 ... 4.70102789e-17\n",
            "   2.62677857e-12 8.50610814e-15]]\n",
            "\n",
            " [[3.58424842e-07 3.65617953e-12 2.15637010e-05 ... 5.57074511e-16\n",
            "   1.82741951e-12 2.01397518e-17]\n",
            "  [3.68991709e-06 5.75680545e-11 1.98717046e-07 ... 1.78525389e-11\n",
            "   3.41427026e-11 1.18560879e-12]\n",
            "  [1.65796839e-04 1.04214370e-09 2.31236257e-08 ... 1.98251406e-08\n",
            "   6.01273087e-10 1.79263804e-08]\n",
            "  [9.99774456e-01 9.61250177e-12 2.56565968e-09 ... 2.83401693e-13\n",
            "   9.38528075e-12 1.97310853e-12]\n",
            "  [9.99993384e-01 3.42628920e-13 7.48065845e-11 ... 1.16913366e-14\n",
            "   3.28391258e-13 5.41190966e-14]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[3.99332745e-09 2.36509944e-11 6.02931305e-09 ... 1.13258903e-13\n",
            "   1.63584597e-11 7.32163353e-13]\n",
            "  [6.53195775e-06 8.93294716e-10 5.50566619e-06 ... 2.22283758e-12\n",
            "   4.65722239e-10 5.83128823e-11]\n",
            "  [1.22187696e-02 2.08624362e-09 1.05198378e-05 ... 5.10909414e-12\n",
            "   1.84058035e-09 6.27562724e-10]\n",
            "  [9.96160209e-01 5.20524041e-11 1.38500047e-08 ... 3.61309053e-14\n",
            "   4.85539109e-11 2.45246792e-11]\n",
            "  [9.99934137e-01 1.53536580e-12 7.81212109e-11 ... 1.15589779e-15\n",
            "   1.63436972e-12 1.19800871e-12]]\n",
            "\n",
            " [[1.15302726e-06 3.25806471e-09 1.00972702e-05 ... 2.21979444e-13\n",
            "   3.10489057e-09 6.95223046e-12]\n",
            "  [4.09461136e-05 4.20950652e-09 2.81227659e-03 ... 2.53581959e-13\n",
            "   3.37873884e-09 3.11799501e-12]\n",
            "  [1.15679555e-04 3.96054345e-09 7.68921256e-01 ... 1.48764334e-11\n",
            "   3.20837357e-09 3.48427433e-11]\n",
            "  [5.96858971e-02 8.92290473e-08 3.49470675e-02 ... 1.94408073e-10\n",
            "   1.04354257e-07 3.05305514e-09]\n",
            "  [9.99384582e-01 1.03258319e-10 1.98537828e-06 ... 6.33203749e-14\n",
            "   1.01025376e-10 4.71669741e-13]]\n",
            "\n",
            " [[9.11485287e-08 7.84290244e-10 9.72513250e-11 ... 7.40320513e-11\n",
            "   7.21585569e-10 5.39447764e-10]\n",
            "  [9.24101187e-06 2.23968205e-10 2.26013356e-07 ... 1.80189835e-10\n",
            "   1.60363237e-10 1.83848228e-10]\n",
            "  [1.93757378e-02 9.05366893e-09 2.85361814e-08 ... 8.24416713e-09\n",
            "   9.17914011e-09 3.76744396e-08]\n",
            "  [9.99677300e-01 9.46262600e-12 4.37570380e-09 ... 8.72405718e-14\n",
            "   1.11108059e-11 1.06617818e-12]\n",
            "  [9.99969125e-01 1.11083213e-12 1.59558075e-10 ... 2.89364581e-14\n",
            "   1.51783241e-12 6.03012052e-13]]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "index_to_word = {index: word for word, index in eng_tokenizer.word_index.items()}\n"
      ],
      "metadata": {
        "id": "uf2D-xTn-Qf0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tCVLqAZB0ifl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "translations = []\n",
        "for sentence in translation:\n",
        "  output = ''\n",
        "  for word in sentence:\n",
        "    token = np.argmax(word)\n",
        "    if token == 0:\n",
        "      continue\n",
        "    else:\n",
        "      eng_word = index_to_word[token]\n",
        "    output+=eng_word\n",
        "  translations.append(output)\n",
        "\n",
        "print(translations)\n",
        "\n",
        "true = []\n",
        "for sentence in test_y:\n",
        "  output = []\n",
        "  for word in sentence:\n",
        "    token = np.argmax(word)\n",
        "    if token == 0:\n",
        "      continue\n",
        "    else:\n",
        "      eng_word = index_to_word[token]\n",
        "    output+= eng_word\n",
        "  true.append(output)\n",
        "\n",
        "\n",
        "results = pd.DataFrame({'true': true, 'trans': translations})\n",
        "\n"
      ],
      "metadata": {
        "id": "EK7G-IA8_On_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results.to_csv('/content/results_string.csv', index=False)"
      ],
      "metadata": {
        "id": "SULLNnYIWVl4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "results = pd.read_csv('/content/results.csv')\n"
      ],
      "metadata": {
        "id": "5fBm2ev0GVh_"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "I have an older version of the results df downloaded and had to restart the runtime, so I will do some preprocessing again."
      ],
      "metadata": {
        "id": "sAM-xvlAJsP3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import ast\n",
        "results['trans'] = results['trans'].apply(ast.literal_eval)\n",
        "results['true'] = results['true'].apply(ast.literal_eval)"
      ],
      "metadata": {
        "id": "eiUY24LBZYCy"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results['trans']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "USHLtvP5KUJF",
        "outputId": "e82a5cd0-5423-4ff7-b68e-b951d918f92b"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0       [i, wanted, to, sing, ]\n",
              "1         [i, hope, you, die, ]\n",
              "2         [you, are, young, , ]\n",
              "3          [it, is, too, hot, ]\n",
              "4          [we, will, wait, , ]\n",
              "                 ...           \n",
              "1981     [i, am, a, sculptor, ]\n",
              "1982       [give, me, half, , ]\n",
              "1983           [go, go, go, , ]\n",
              "1984    [what, will, i, have, ]\n",
              "1985      [open, a, window, , ]\n",
              "Name: trans, Length: 1986, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trans = results['trans']\n",
        "\n",
        "translations = []\n",
        "for row in trans:\n",
        "  string = ''\n",
        "  for token in row:\n",
        "    if token == '':\n",
        "      continue\n",
        "    if token == 'ca': #in the preprocessing process I accidently replaced can't with ca not\n",
        "      token = 'can'\n",
        "    if token == 'wo': #in the preprocessing process I accidently replaced won't with wo not\n",
        "      token = 'will'\n",
        "\n",
        "    # we will get rid of some clearly equivlant translations, such as 'a' and 'the' being equivlant in russian\n",
        "\n",
        "    if token == 'a':\n",
        "      token = 'the'\n",
        "\n",
        "    string+= token + ' '\n",
        "  translations.append(string)\n",
        "\n",
        "true = results['true']\n",
        "\n",
        "trues = []\n",
        "for row in true:\n",
        "  string = ''\n",
        "  for token in row:\n",
        "    if token == '':\n",
        "      continue\n",
        "    if token == 'ca': #in the preprocessing process I accidently replaced can't with ca not\n",
        "      token = 'can'\n",
        "    if token == 'wo': #in the preprocessing process I accidently replaced won't with wo not\n",
        "      token = 'will'\n",
        "\n",
        "    # we will get rid of some clearly equivlant translations, such as 'a' and 'the' being equivlant in russian\n",
        "\n",
        "    if token == 'a':\n",
        "      token = 'the'\n",
        "\n",
        "    string+= token + ' '\n",
        "  trues.append(string)\n",
        "\n",
        "trues = [sentence[:-1] for sentence in trues] # to get rid of extra spasce at the end\n",
        "translations = [sentence[:-1] for sentence in translations]"
      ],
      "metadata": {
        "id": "sQ7NhTdNG6IC"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trues[30:40]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6wOJqlgtOEFS",
        "outputId": "0addfded-74d3-4957-ec88-4f167cd3d38d"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['tom had the gun',\n",
              " 'i am stubborn',\n",
              " 'we can make it',\n",
              " 'i got mad',\n",
              " 'tom was angry',\n",
              " 'i was not alone',\n",
              " 'i read his book',\n",
              " 'cover your legs',\n",
              " 'i sold the book',\n",
              " 'stop quibbling']"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "translations[30:40]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9MKeNrncOG7r",
        "outputId": "eed328c3-a53f-4f6b-e86b-69a3f87c27c0"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['tom had the gun',\n",
              " 'i am contagious',\n",
              " 'we will it it',\n",
              " 'i was myself',\n",
              " 'tom was angry',\n",
              " 'i was not alone',\n",
              " 'i read it book',\n",
              " 'cover your feet',\n",
              " 'i took the book',\n",
              " 'toms famous']"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install evaluate"
      ],
      "metadata": {
        "id": "FN_dikNiS9HE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import evaluate\n",
        "bleu = evaluate.load('bleu')\n",
        "predictions = translations\n",
        "references = trues\n",
        "results = bleu.compute(predictions=predictions, references=references,\n",
        "          max_order = 2) # the sentences are very short so we will only use 2 n-grams\n",
        "print(results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CkwuDjtISiDN",
        "outputId": "3cd4953a-30e2-414e-a40a-67715caa9013"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'bleu': 0.5969121417304739, 'precisions': [0.695322376738306, 0.526715799170889], 'brevity_penalty': 0.9863456581662835, 'length_ratio': 0.9864380358534685, 'translation_length': 6328, 'reference_length': 6415}\n"
          ]
        }
      ]
    }
  ]
}