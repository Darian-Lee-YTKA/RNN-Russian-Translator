{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMJzCsALlWarVRgpk608ce7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Darian-Lee-YTKA/RNN-Russian-Translator/blob/main/simple_translator.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pymorphy2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qeirRnJx525-",
        "outputId": "6da90766-3016-4e13-b645-1d55e1bf82d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pymorphy2\n",
            "  Downloading pymorphy2-0.9.1-py3-none-any.whl (55 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.5/55.5 kB\u001b[0m \u001b[31m825.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting dawg-python>=0.7.1 (from pymorphy2)\n",
            "  Downloading DAWG_Python-0.7.2-py2.py3-none-any.whl (11 kB)\n",
            "Collecting pymorphy2-dicts-ru<3.0,>=2.4 (from pymorphy2)\n",
            "  Downloading pymorphy2_dicts_ru-2.4.417127.4579844-py2.py3-none-any.whl (8.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.2/8.2 MB\u001b[0m \u001b[31m26.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting docopt>=0.6 (from pymorphy2)\n",
            "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: docopt\n",
            "  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13706 sha256=8f03b6e484dbe4bcf1bff75beeef671e224a7d45ac954b83aebef01b4466f811\n",
            "  Stored in directory: /root/.cache/pip/wheels/fc/ab/d4/5da2067ac95b36618c629a5f93f809425700506f72c9732fac\n",
            "Successfully built docopt\n",
            "Installing collected packages: pymorphy2-dicts-ru, docopt, dawg-python, pymorphy2\n",
            "Successfully installed dawg-python-0.7.2 docopt-0.6.2 pymorphy2-0.9.1 pymorphy2-dicts-ru-2.4.417127.4579844\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iOk3lt0k2ZSN",
        "outputId": "0cfd614f-1464-4c30-b8e8-5632226517fa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import pymorphy2\n",
        "import random\n",
        "import nltk\n",
        "import re\n",
        "import string\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt')\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "with open('/content/rus.txt', 'r') as file:\n",
        "    content = file.readlines()\n",
        "\n",
        "def remove_punct(line):\n",
        "    line = line.replace(\"n't\", \" not\") # so conjuctions can be tokenized more easily\n",
        "    line = line.replace(\"'m\", \" am\")\n",
        "    line = line.replace(\"let's\", \"let us\")\n",
        "    line = line.replace(\"'re\", \" are\")\n",
        "    line = line.replace(\"Tom's\", \"Toms\") # so possestive doesn't become \"is\"\n",
        "    line = line.replace(\"'s\", \" is\")\n",
        "    line = line.replace(\"'ve\", \" have\")\n",
        "    line = line.replace(\"'ll\", \" will\")\n",
        "    line = line.replace(\"won't\", \" will not\")\n",
        "    line = line.replace(\"'\", \" \")\n",
        "\n",
        "    # I am going to keep the question make in since it effects word order\n",
        "    punct_except_question_mark = ''.join(char for char in string.punctuation if char != '?')\n",
        "    punct_regex = re.compile('[%s]' % re.escape(punct_except_question_mark))\n",
        "    line = punct_regex.sub('', line)\n",
        "    return line\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "russian_list = []\n",
        "english_list = []\n",
        "\n",
        "# the lines are grouped by complexity.\n",
        "\n",
        "\n",
        "for line in content[:25000]: #only do the first 25,000 to keep the model a more managable size\n",
        "\n",
        "# NOTE: this size will likely shrink even more once we delete translations that become duplicates once the Russian is lematized\n",
        "\n",
        "    line = re.sub(r'\\tCC-BY.*', '', line) #get rid of everything that is not data\n",
        "    line = re.sub(r'\\t', 'β', line) #make the lines easier to split. I was getting errors doing line.split based on tab,\n",
        "    # so I replaced the tabs with beta\n",
        "\n",
        "    line = remove_punct(line)\n",
        "\n",
        "    eng, rus = line.split('β')\n",
        "\n",
        "    rus = rus.strip().lower()  #to get rid of /n at end and make it all the same case\n",
        "\n",
        "    eng = eng.strip().lower()\n",
        "\n",
        "    russian_list.append(rus+' ')  #this will come in handy later when we add 'p' to tag past tense\n",
        "\n",
        "    english_list.append(eng)\n",
        "\n",
        "\n",
        "\n",
        "data = pd.DataFrame({'rus': russian_list, 'eng': english_list})\n",
        "data = data.sample(frac=1, random_state=12) #shuffle it\n",
        "russian_list = data['rus'].tolist()\n",
        "english_list = data['eng'].tolist()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uHB3HC3mhbak",
        "outputId": "bb1dc866-fd59-4af0-ab28-511f7f25f745"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(25000, 2)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install spacy\n",
        "!python -m spacy download ru_core_news_sm"
      ],
      "metadata": {
        "id": "kkJel65bEpp3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(russian_list)): #for tagging past tense\n",
        "# in russian, past tense is super easy to detect for most words because it will have one of these endings\n",
        "    russian_list[i] = re.sub(r'л(?=\\s|$)', 'л p', russian_list[i])\n",
        "    russian_list[i] = re.sub(r'ла(?=\\s|$)', 'ла p', russian_list[i])\n",
        "    russian_list[i] = re.sub(r'лся(?=\\s|$)', 'лся p', russian_list[i])\n",
        "    russian_list[i] = re.sub(r'лась(?=\\s|$)', 'лась p', russian_list[i])\n",
        "    russian_list[i] = re.sub(r'ло(?=\\s|$)', 'ло p', russian_list[i])\n",
        "    russian_list[i] = re.sub(r'лось(?=\\s|$)', 'лось p', russian_list[i])\n",
        "    russian_list[i] = re.sub(r'ли(?=\\s|$)', 'ли p', russian_list[i])\n",
        "    russian_list[i] = re.sub(r'лись(?=\\s|$)', 'лись p', russian_list[i])\n",
        "    russian_list[i] = re.sub(r'\\s+$', '', russian_list[i], flags=re.MULTILINE)\n"
      ],
      "metadata": {
        "id": "_Q_5NVWsGHCU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "russian_list"
      ],
      "metadata": {
        "id": "ezfR2hF5Goky"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "russian_lemmas = []\n",
        "\n",
        "nlp = spacy.load(\"ru_core_news_sm\")\n",
        "\n",
        "counter = 0\n",
        "for line in russian_list:\n",
        "    counter+= 1\n",
        "    print(counter) # so I can get an estimate for how much longer it will take by watching the numbers increase\n",
        "    doc = nlp(line)\n",
        "    lemmas = [token.lemma_ if token.text != 'p' else 'p' for token in doc] #so that the past tense remains tagged\n",
        "    russian_lemmas.append(lemmas)\n",
        "\n",
        "\n",
        "print(russian_lemmas)\n",
        "# I'm not gonna include the output in github because it is really long from all the print statements"
      ],
      "metadata": {
        "id": "ESMNphhcI5BV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow"
      ],
      "metadata": {
        "id": "JcHpnH8OOGtV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# this will introduce duplicates but we will delete them later\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "rus_tokenizer = Tokenizer(oov_token=\"<OOV>\") #for handeling unknown tokens\n",
        "eng_tokenizer = Tokenizer(oov_token=\"<OOV>\")\n",
        "\n",
        "\n",
        "rus_tokenizer.fit_on_texts(russian_lemmas)\n",
        "\n",
        "rus_sequences = rus_tokenizer.texts_to_sequences(russian_lemmas)\n",
        "\n",
        "rus_vocab_size = len(rus_tokenizer.word_index) + 1\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "eng_tokenizer.fit_on_texts(english_list)\n",
        "\n",
        "eng_sequences = eng_tokenizer.texts_to_sequences(english_list)\n",
        "\n",
        "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
        "\n",
        "final_data = pd.DataFrame({'rus': rus_sequences, 'eng': eng_sequences})\n",
        "\n",
        "\n",
        "# as promised, we are deleting the dups\n",
        "final_data = final_data.drop_duplicates(subset='rus', keep='first')\n",
        "\n",
        "\n",
        "# so that we can do this in a seperate runtime to preserve RAM on google collab\n",
        "import json\n",
        "\n",
        "with open('/content/rus_tokenizer.json', 'w') as file:\n",
        "    file.write(rus_tokenizer.to_json())\n",
        "\n",
        "with open('/content/eng_tokenizer.json', 'w') as file:\n",
        "    file.write(eng_tokenizer.to_json())"
      ],
      "metadata": {
        "id": "GtcOz4liMReA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "final_data['rus_lens'] = final_data['rus'].apply(lambda x: len(x))\n",
        "\n",
        "max_seq_rus = max(final_data['rus_lens'])\n",
        "print(max_seq_rus)\n",
        "\n",
        "final_data['eng_lens'] = final_data['eng'].apply(lambda x: len(x))\n",
        "\n",
        "max_seq_eng = max(final_data['eng_lens'])\n",
        "print(max_seq_rus)\n"
      ],
      "metadata": {
        "id": "Y0TjOBxThIMO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_data['padded_rus'] = final_data['rus'].apply(lambda x: pad_sequences([x], maxlen=max_seq_rus, padding='post', truncating='post')[0])\n",
        "final_data['padded_eng'] = final_data['eng'].apply(lambda x: pad_sequences([x], maxlen=max_seq_eng, padding='post', truncating='post')[0])\n",
        "\n"
      ],
      "metadata": {
        "id": "oVqTgBbVivXj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# one hot encode the output:\n",
        "\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "y = final_data['padded_eng'].apply(lambda x: to_categorical(x, num_classes=eng_vocab_size))\n",
        "\n"
      ],
      "metadata": {
        "id": "WsxupR6FlDQu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "y_array = np.array(y.tolist())\n",
        "print(y_array.shape)\n",
        "y_reshaped = y_array.reshape(y_array.shape[0], y_array.shape[1], -1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aLIv3SryxDQl",
        "outputId": "a4fd21fd-9ead-4cf2-e5ab-2117345ae6ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(19851, 5, 3212)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_array.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iM-DI0wqpQxO",
        "outputId": "0bee5a7a-224d-44a2-bab9-2e813e2f2927"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(19851, 5, 3212)"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EUKhb6_YfJQ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X = final_data['padded_rus']\n",
        "y = y_reshaped\n",
        "\n",
        "for x in y[:10]:\n",
        "  print(len(x[0]))\n",
        "\n",
        "temp_X, test_X, temp_y, test_y = train_test_split(X, y, test_size=0.1, random_state=24)\n",
        "\n",
        "\n",
        "train_X, val_X, train_y, val_y = train_test_split(temp_X, temp_y, test_size=0.1, random_state=24)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LLc690A7Cozq",
        "outputId": "4bf1a5c3-06d0-4c82-b439-cadaa2ceae23"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3212\n",
            "3212\n",
            "3212\n",
            "3212\n",
            "3212\n",
            "3212\n",
            "3212\n",
            "3212\n",
            "3212\n",
            "3212\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "train_X = np.array(train_X.values.tolist())\n",
        "\n",
        "train_X = train_X.astype(np.float32)\n",
        "\n",
        "test_X = np.array(test_X.values.tolist())\n",
        "\n",
        "test_X = test_X.astype(np.float32)\n",
        "\n",
        "val_X = np.array(val_X.values.tolist())\n",
        "\n",
        "val_X = val_X.astype(np.float32)\n",
        "\n"
      ],
      "metadata": {
        "id": "crn__PDXj1eK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_X.shape\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9dCJyTSbwHC9",
        "outputId": "983b2887-0aed-4a40-c1a3-699a2f3268c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(16078, 10)"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_y.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nGbihF5KwL-s",
        "outputId": "fbfb8d10-f978-44d6-c95d-91e184c73de3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(16078, 5, 3212)"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_X"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QKgC_QVxp242",
        "outputId": "a2f90d6f-4cfc-4db0-a0c3-b94bb6864fae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[  2.,  50.,   3., ...,   0.,   0.,   0.],\n",
              "       [987.,   0.,   0., ...,   0.,   0.,   0.],\n",
              "       [403.,  18., 179., ...,   0.,   0.,   0.],\n",
              "       ...,\n",
              "       [ 64.,  39.,   3., ...,   0.,   0.,   0.],\n",
              "       [195.,  16.,   5., ...,   0.,   0.,   0.],\n",
              "       [ 16., 116.,  29., ...,   0.,   0.,   0.]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OxUGuZJmp4gF",
        "outputId": "da085d46-1376-464d-edcb-986be79e801d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[[0., 0., 1., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 1., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.]],\n",
              "\n",
              "       [[0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [1., 0., 0., ..., 0., 0., 0.],\n",
              "        [1., 0., 0., ..., 0., 0., 0.],\n",
              "        [1., 0., 0., ..., 0., 0., 0.]],\n",
              "\n",
              "       [[0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [1., 0., 0., ..., 0., 0., 0.],\n",
              "        [1., 0., 0., ..., 0., 0., 0.]],\n",
              "\n",
              "       ...,\n",
              "\n",
              "       [[0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [1., 0., 0., ..., 0., 0., 0.],\n",
              "        [1., 0., 0., ..., 0., 0., 0.]],\n",
              "\n",
              "       [[0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [1., 0., 0., ..., 0., 0., 0.],\n",
              "        [1., 0., 0., ..., 0., 0., 0.]],\n",
              "\n",
              "       [[0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [1., 0., 0., ..., 0., 0., 0.],\n",
              "        [1., 0., 0., ..., 0., 0., 0.]]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "ZdpwiAaIQTfD"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1ahklxGiQTRx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# my RAM keeps running out, so this will buy me some time.\n",
        "# I will restart my runtime and pick up from here\n",
        "np.save('/content/train_X.npy', train_X)\n",
        "np.save('/content/train_y.npy', train_y)\n",
        "np.save('/content/test_X.npy', test_X)\n",
        "np.save('/content/test_y.npy', test_y)\n",
        "np.save('/content/val_X.npy', val_X)\n",
        "np.save('/content/val_y.npy', val_y)"
      ],
      "metadata": {
        "id": "dYAmN8SyPBkJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "train_X = np.load('/content/train_X.npy')\n",
        "train_y = np.load('/content/train_y.npy')\n",
        "test_X = np.load('/content/test_X.npy')\n",
        "test_y = np.load('/content/test_y.npy')\n",
        "val_X = np.load('/content/val_X.npy')\n",
        "val_y = np.load('/content/val_y.npy')"
      ],
      "metadata": {
        "id": "nBEO-8ZaQdBf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.text import tokenizer_from_json\n",
        "\n",
        "\n",
        "with open('/content/rus_tokenizer.json', 'r') as file:\n",
        "    rus_tokenizer = tokenizer_from_json(file.read())\n",
        "\n",
        "with open('/content/eng_tokenizer.json', 'r') as file:\n",
        "    eng_tokenizer = tokenizer_from_json(file.read())\n",
        "\n",
        "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
        "rus_vocab_size = len(rus_tokenizer.word_index) + 1"
      ],
      "metadata": {
        "id": "wW2Z-9gjRhn3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, RepeatVector, TimeDistributed, Dense\n",
        "from tensorflow.keras.utils import plot_model\n",
        "\n",
        "def create_model(src_vocab, tar_vocab, src_timesteps, tar_timesteps, n_units):\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(src_vocab, n_units, input_length=src_timesteps, mask_zero=True))\n",
        "    model.add(LSTM(n_units))\n",
        "    model.add(RepeatVector(tar_timesteps))\n",
        "    model.add(LSTM(n_units, return_sequences=True))\n",
        "    model.add(TimeDistributed(Dense(tar_vocab, activation='softmax')))\n",
        "    model.compile(optimizer='adam', loss='categorical_crossentropy')\n",
        "    return model\n",
        "\n",
        "\n",
        "\n",
        "units = [128, 256]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Om8I3UAR0L1b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(rus_vocab_size)\n",
        "print(eng_vocab_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y869PN_73gQ8",
        "outputId": "b874d9ec-2282-42c6-c3f8-973ea8e163c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5660\n",
            "3212\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.callbacks import ModelCheckpoint\n",
        "best_model = None\n",
        "best_val_loss = float('inf')\n",
        "for unit in units:\n",
        "  model = create_model(rus_vocab_size, eng_vocab_size, 10, 5, unit)\n",
        "  print(model.summary())\n",
        "  filename = f'/content/best_model_{unit}.h5'\n",
        "  checkpoint = ModelCheckpoint(filename, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
        "  model.fit(train_X, train_y, epochs=30, batch_size=100, validation_data=(val_X, val_y), callbacks=[checkpoint], verbose=1)\n",
        "  eval_loss = model.evaluate(test_X, test_y, verbose=1)\n",
        "\n",
        "  print(f\"testing loss for unit {unit}: {eval_loss}\")\n",
        "\n",
        "  if eval_loss < best_val_loss:\n",
        "      best_val_loss = eval_loss\n",
        "      best_model = model\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hJCQN53tNPqW",
        "outputId": "c1e56129-095a-4367-c5da-cdf1bf5c54e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_1 (Embedding)     (None, 10, 128)           724480    \n",
            "                                                                 \n",
            " lstm_2 (LSTM)               (None, 128)               131584    \n",
            "                                                                 \n",
            " repeat_vector_1 (RepeatVec  (None, 5, 128)            0         \n",
            " tor)                                                            \n",
            "                                                                 \n",
            " lstm_3 (LSTM)               (None, 5, 128)            131584    \n",
            "                                                                 \n",
            " time_distributed_1 (TimeDi  (None, 5, 3212)           414348    \n",
            " stributed)                                                      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1401996 (5.35 MB)\n",
            "Trainable params: 1401996 (5.35 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/30\n",
            "161/161 [==============================] - ETA: 0s - loss: 4.9311\n",
            "Epoch 1: val_loss improved from inf to 4.21917, saving model to /content/best_model_128.h5\n",
            "161/161 [==============================] - 43s 217ms/step - loss: 4.9311 - val_loss: 4.2192\n",
            "Epoch 2/30\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "161/161 [==============================] - ETA: 0s - loss: 4.0593\n",
            "Epoch 2: val_loss improved from 4.21917 to 3.97934, saving model to /content/best_model_128.h5\n",
            "161/161 [==============================] - 35s 219ms/step - loss: 4.0593 - val_loss: 3.9793\n",
            "Epoch 3/30\n",
            "161/161 [==============================] - ETA: 0s - loss: 3.8375\n",
            "Epoch 3: val_loss improved from 3.97934 to 3.80126, saving model to /content/best_model_128.h5\n",
            "161/161 [==============================] - 28s 173ms/step - loss: 3.8375 - val_loss: 3.8013\n",
            "Epoch 4/30\n",
            "161/161 [==============================] - ETA: 0s - loss: 3.6910\n",
            "Epoch 4: val_loss improved from 3.80126 to 3.70300, saving model to /content/best_model_128.h5\n",
            "161/161 [==============================] - 28s 176ms/step - loss: 3.6910 - val_loss: 3.7030\n",
            "Epoch 5/30\n",
            "161/161 [==============================] - ETA: 0s - loss: 3.5932\n",
            "Epoch 5: val_loss improved from 3.70300 to 3.61697, saving model to /content/best_model_128.h5\n",
            "161/161 [==============================] - 32s 196ms/step - loss: 3.5932 - val_loss: 3.6170\n",
            "Epoch 6/30\n",
            "161/161 [==============================] - ETA: 0s - loss: 3.4689\n",
            "Epoch 6: val_loss improved from 3.61697 to 3.43295, saving model to /content/best_model_128.h5\n",
            "161/161 [==============================] - 28s 173ms/step - loss: 3.4689 - val_loss: 3.4329\n",
            "Epoch 7/30\n",
            "161/161 [==============================] - ETA: 0s - loss: 3.2741\n",
            "Epoch 7: val_loss improved from 3.43295 to 3.26979, saving model to /content/best_model_128.h5\n",
            "161/161 [==============================] - 28s 173ms/step - loss: 3.2741 - val_loss: 3.2698\n",
            "Epoch 8/30\n",
            "161/161 [==============================] - ETA: 0s - loss: 3.1104\n",
            "Epoch 8: val_loss improved from 3.26979 to 3.14416, saving model to /content/best_model_128.h5\n",
            "161/161 [==============================] - 28s 176ms/step - loss: 3.1104 - val_loss: 3.1442\n",
            "Epoch 9/30\n",
            "161/161 [==============================] - ETA: 0s - loss: 2.9621\n",
            "Epoch 9: val_loss improved from 3.14416 to 3.03104, saving model to /content/best_model_128.h5\n",
            "161/161 [==============================] - 28s 173ms/step - loss: 2.9621 - val_loss: 3.0310\n",
            "Epoch 10/30\n",
            "161/161 [==============================] - ETA: 0s - loss: 2.8176\n",
            "Epoch 10: val_loss improved from 3.03104 to 2.91406, saving model to /content/best_model_128.h5\n",
            "161/161 [==============================] - 28s 174ms/step - loss: 2.8176 - val_loss: 2.9141\n",
            "Epoch 11/30\n",
            "161/161 [==============================] - ETA: 0s - loss: 2.6780\n",
            "Epoch 11: val_loss improved from 2.91406 to 2.78544, saving model to /content/best_model_128.h5\n",
            "161/161 [==============================] - 29s 178ms/step - loss: 2.6780 - val_loss: 2.7854\n",
            "Epoch 12/30\n",
            "161/161 [==============================] - ETA: 0s - loss: 2.5419\n",
            "Epoch 12: val_loss improved from 2.78544 to 2.68612, saving model to /content/best_model_128.h5\n",
            "161/161 [==============================] - 31s 192ms/step - loss: 2.5419 - val_loss: 2.6861\n",
            "Epoch 13/30\n",
            "161/161 [==============================] - ETA: 0s - loss: 2.4200\n",
            "Epoch 13: val_loss improved from 2.68612 to 2.59549, saving model to /content/best_model_128.h5\n",
            "161/161 [==============================] - 29s 180ms/step - loss: 2.4200 - val_loss: 2.5955\n",
            "Epoch 14/30\n",
            "161/161 [==============================] - ETA: 0s - loss: 2.3092\n",
            "Epoch 14: val_loss improved from 2.59549 to 2.51769, saving model to /content/best_model_128.h5\n",
            "161/161 [==============================] - 27s 167ms/step - loss: 2.3092 - val_loss: 2.5177\n",
            "Epoch 15/30\n",
            "161/161 [==============================] - ETA: 0s - loss: 2.2054\n",
            "Epoch 15: val_loss improved from 2.51769 to 2.43873, saving model to /content/best_model_128.h5\n",
            "161/161 [==============================] - 27s 170ms/step - loss: 2.2054 - val_loss: 2.4387\n",
            "Epoch 16/30\n",
            "161/161 [==============================] - ETA: 0s - loss: 2.1070\n",
            "Epoch 16: val_loss improved from 2.43873 to 2.37349, saving model to /content/best_model_128.h5\n",
            "161/161 [==============================] - 27s 166ms/step - loss: 2.1070 - val_loss: 2.3735\n",
            "Epoch 17/30\n",
            "161/161 [==============================] - ETA: 0s - loss: 2.0156\n",
            "Epoch 17: val_loss improved from 2.37349 to 2.31572, saving model to /content/best_model_128.h5\n",
            "161/161 [==============================] - 27s 169ms/step - loss: 2.0156 - val_loss: 2.3157\n",
            "Epoch 18/30\n",
            "161/161 [==============================] - ETA: 0s - loss: 1.9268\n",
            "Epoch 18: val_loss improved from 2.31572 to 2.26939, saving model to /content/best_model_128.h5\n",
            "161/161 [==============================] - 27s 166ms/step - loss: 1.9268 - val_loss: 2.2694\n",
            "Epoch 19/30\n",
            "161/161 [==============================] - ETA: 0s - loss: 1.8457\n",
            "Epoch 19: val_loss improved from 2.26939 to 2.20963, saving model to /content/best_model_128.h5\n",
            "161/161 [==============================] - 29s 179ms/step - loss: 1.8457 - val_loss: 2.2096\n",
            "Epoch 20/30\n",
            "161/161 [==============================] - ETA: 0s - loss: 1.7649\n",
            "Epoch 20: val_loss improved from 2.20963 to 2.17149, saving model to /content/best_model_128.h5\n",
            "161/161 [==============================] - 34s 213ms/step - loss: 1.7649 - val_loss: 2.1715\n",
            "Epoch 21/30\n",
            "161/161 [==============================] - ETA: 0s - loss: 1.6862\n",
            "Epoch 21: val_loss improved from 2.17149 to 2.11743, saving model to /content/best_model_128.h5\n",
            "161/161 [==============================] - 28s 173ms/step - loss: 1.6862 - val_loss: 2.1174\n",
            "Epoch 22/30\n",
            "161/161 [==============================] - ETA: 0s - loss: 1.6104\n",
            "Epoch 22: val_loss improved from 2.11743 to 2.07183, saving model to /content/best_model_128.h5\n",
            "161/161 [==============================] - 28s 172ms/step - loss: 1.6104 - val_loss: 2.0718\n",
            "Epoch 23/30\n",
            "161/161 [==============================] - ETA: 0s - loss: 1.5383\n",
            "Epoch 23: val_loss improved from 2.07183 to 2.03266, saving model to /content/best_model_128.h5\n",
            "161/161 [==============================] - 29s 177ms/step - loss: 1.5383 - val_loss: 2.0327\n",
            "Epoch 24/30\n",
            "161/161 [==============================] - ETA: 0s - loss: 1.4665\n",
            "Epoch 24: val_loss improved from 2.03266 to 2.00058, saving model to /content/best_model_128.h5\n",
            "161/161 [==============================] - 37s 232ms/step - loss: 1.4665 - val_loss: 2.0006\n",
            "Epoch 25/30\n",
            "161/161 [==============================] - ETA: 0s - loss: 1.3963\n",
            "Epoch 25: val_loss improved from 2.00058 to 1.96450, saving model to /content/best_model_128.h5\n",
            "161/161 [==============================] - 34s 208ms/step - loss: 1.3963 - val_loss: 1.9645\n",
            "Epoch 26/30\n",
            "161/161 [==============================] - ETA: 0s - loss: 1.3295\n",
            "Epoch 26: val_loss improved from 1.96450 to 1.93263, saving model to /content/best_model_128.h5\n",
            "161/161 [==============================] - 32s 199ms/step - loss: 1.3295 - val_loss: 1.9326\n",
            "Epoch 27/30\n",
            "161/161 [==============================] - ETA: 0s - loss: 1.2598\n",
            "Epoch 27: val_loss improved from 1.93263 to 1.90479, saving model to /content/best_model_128.h5\n",
            "161/161 [==============================] - 29s 177ms/step - loss: 1.2598 - val_loss: 1.9048\n",
            "Epoch 28/30\n",
            "161/161 [==============================] - ETA: 0s - loss: 1.1932\n",
            "Epoch 28: val_loss improved from 1.90479 to 1.86790, saving model to /content/best_model_128.h5\n",
            "161/161 [==============================] - 29s 180ms/step - loss: 1.1932 - val_loss: 1.8679\n",
            "Epoch 29/30\n",
            "161/161 [==============================] - ETA: 0s - loss: 1.1306\n",
            "Epoch 29: val_loss improved from 1.86790 to 1.84418, saving model to /content/best_model_128.h5\n",
            "161/161 [==============================] - 28s 174ms/step - loss: 1.1306 - val_loss: 1.8442\n",
            "Epoch 30/30\n",
            "161/161 [==============================] - ETA: 0s - loss: 1.0683\n",
            "Epoch 30: val_loss improved from 1.84418 to 1.81930, saving model to /content/best_model_128.h5\n",
            "161/161 [==============================] - 28s 177ms/step - loss: 1.0683 - val_loss: 1.8193\n",
            "63/63 [==============================] - 1s 20ms/step - loss: 1.8588\n",
            "testing loss for unit 128: 1.8587579727172852\n",
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_2 (Embedding)     (None, 10, 256)           1448960   \n",
            "                                                                 \n",
            " lstm_4 (LSTM)               (None, 256)               525312    \n",
            "                                                                 \n",
            " repeat_vector_2 (RepeatVec  (None, 5, 256)            0         \n",
            " tor)                                                            \n",
            "                                                                 \n",
            " lstm_5 (LSTM)               (None, 5, 256)            525312    \n",
            "                                                                 \n",
            " time_distributed_2 (TimeDi  (None, 5, 3212)           825484    \n",
            " stributed)                                                      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 3325068 (12.68 MB)\n",
            "Trainable params: 3325068 (12.68 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/30\n",
            "161/161 [==============================] - ETA: 0s - loss: 4.4793\n",
            "Epoch 1: val_loss improved from inf to 3.65278, saving model to /content/best_model_256.h5\n",
            "161/161 [==============================] - 66s 361ms/step - loss: 4.4793 - val_loss: 3.6528\n",
            "Epoch 2/30\n",
            "161/161 [==============================] - ETA: 0s - loss: 3.4737\n",
            "Epoch 2: val_loss improved from 3.65278 to 3.39172, saving model to /content/best_model_256.h5\n",
            "161/161 [==============================] - 56s 345ms/step - loss: 3.4737 - val_loss: 3.3917\n",
            "Epoch 3/30\n",
            "161/161 [==============================] - ETA: 0s - loss: 3.2468\n",
            "Epoch 3: val_loss improved from 3.39172 to 3.19676, saving model to /content/best_model_256.h5\n",
            "161/161 [==============================] - 55s 344ms/step - loss: 3.2468 - val_loss: 3.1968\n",
            "Epoch 4/30\n",
            "161/161 [==============================] - ETA: 0s - loss: 3.0042\n",
            "Epoch 4: val_loss improved from 3.19676 to 2.98858, saving model to /content/best_model_256.h5\n",
            "161/161 [==============================] - 55s 344ms/step - loss: 3.0042 - val_loss: 2.9886\n",
            "Epoch 5/30\n",
            "161/161 [==============================] - ETA: 0s - loss: 2.7697\n",
            "Epoch 5: val_loss improved from 2.98858 to 2.79306, saving model to /content/best_model_256.h5\n",
            "161/161 [==============================] - 61s 381ms/step - loss: 2.7697 - val_loss: 2.7931\n",
            "Epoch 6/30\n",
            "161/161 [==============================] - ETA: 0s - loss: 2.5378\n",
            "Epoch 6: val_loss improved from 2.79306 to 2.61083, saving model to /content/best_model_256.h5\n",
            "161/161 [==============================] - 56s 347ms/step - loss: 2.5378 - val_loss: 2.6108\n",
            "Epoch 7/30\n",
            "161/161 [==============================] - ETA: 0s - loss: 2.3413\n",
            "Epoch 7: val_loss improved from 2.61083 to 2.49422, saving model to /content/best_model_256.h5\n",
            "161/161 [==============================] - 56s 348ms/step - loss: 2.3413 - val_loss: 2.4942\n",
            "Epoch 8/30\n",
            "161/161 [==============================] - ETA: 0s - loss: 2.1741\n",
            "Epoch 8: val_loss improved from 2.49422 to 2.38083, saving model to /content/best_model_256.h5\n",
            "161/161 [==============================] - 59s 365ms/step - loss: 2.1741 - val_loss: 2.3808\n",
            "Epoch 9/30\n",
            "161/161 [==============================] - ETA: 0s - loss: 2.0254\n",
            "Epoch 9: val_loss improved from 2.38083 to 2.28235, saving model to /content/best_model_256.h5\n",
            "161/161 [==============================] - 57s 354ms/step - loss: 2.0254 - val_loss: 2.2823\n",
            "Epoch 10/30\n",
            "161/161 [==============================] - ETA: 0s - loss: 1.8927\n",
            "Epoch 10: val_loss improved from 2.28235 to 2.19667, saving model to /content/best_model_256.h5\n",
            "161/161 [==============================] - 55s 343ms/step - loss: 1.8927 - val_loss: 2.1967\n",
            "Epoch 11/30\n",
            "161/161 [==============================] - ETA: 0s - loss: 1.7670\n",
            "Epoch 11: val_loss improved from 2.19667 to 2.12439, saving model to /content/best_model_256.h5\n",
            "161/161 [==============================] - 58s 359ms/step - loss: 1.7670 - val_loss: 2.1244\n",
            "Epoch 12/30\n",
            "161/161 [==============================] - ETA: 0s - loss: 1.6502\n",
            "Epoch 12: val_loss improved from 2.12439 to 2.06755, saving model to /content/best_model_256.h5\n",
            "161/161 [==============================] - 57s 351ms/step - loss: 1.6502 - val_loss: 2.0675\n",
            "Epoch 13/30\n",
            "161/161 [==============================] - ETA: 0s - loss: 1.5365\n",
            "Epoch 13: val_loss improved from 2.06755 to 2.00654, saving model to /content/best_model_256.h5\n",
            "161/161 [==============================] - 53s 330ms/step - loss: 1.5365 - val_loss: 2.0065\n",
            "Epoch 14/30\n",
            "161/161 [==============================] - ETA: 0s - loss: 1.4311\n",
            "Epoch 14: val_loss improved from 2.00654 to 1.94317, saving model to /content/best_model_256.h5\n",
            "161/161 [==============================] - 54s 338ms/step - loss: 1.4311 - val_loss: 1.9432\n",
            "Epoch 15/30\n",
            "161/161 [==============================] - ETA: 0s - loss: 1.3294\n",
            "Epoch 15: val_loss improved from 1.94317 to 1.88352, saving model to /content/best_model_256.h5\n",
            "161/161 [==============================] - 57s 354ms/step - loss: 1.3294 - val_loss: 1.8835\n",
            "Epoch 16/30\n",
            "161/161 [==============================] - ETA: 0s - loss: 1.2274\n",
            "Epoch 16: val_loss improved from 1.88352 to 1.83824, saving model to /content/best_model_256.h5\n",
            "161/161 [==============================] - 56s 345ms/step - loss: 1.2274 - val_loss: 1.8382\n",
            "Epoch 17/30\n",
            "161/161 [==============================] - ETA: 0s - loss: 1.1289\n",
            "Epoch 17: val_loss improved from 1.83824 to 1.80229, saving model to /content/best_model_256.h5\n",
            "161/161 [==============================] - 56s 350ms/step - loss: 1.1289 - val_loss: 1.8023\n",
            "Epoch 18/30\n",
            "161/161 [==============================] - ETA: 0s - loss: 1.0396\n",
            "Epoch 18: val_loss improved from 1.80229 to 1.75566, saving model to /content/best_model_256.h5\n",
            "161/161 [==============================] - 57s 357ms/step - loss: 1.0396 - val_loss: 1.7557\n",
            "Epoch 19/30\n",
            "161/161 [==============================] - ETA: 0s - loss: 0.9516\n",
            "Epoch 19: val_loss improved from 1.75566 to 1.73162, saving model to /content/best_model_256.h5\n",
            "161/161 [==============================] - 57s 352ms/step - loss: 0.9516 - val_loss: 1.7316\n",
            "Epoch 20/30\n",
            "161/161 [==============================] - ETA: 0s - loss: 0.8685\n",
            "Epoch 20: val_loss improved from 1.73162 to 1.69169, saving model to /content/best_model_256.h5\n",
            "161/161 [==============================] - 57s 352ms/step - loss: 0.8685 - val_loss: 1.6917\n",
            "Epoch 21/30\n",
            "161/161 [==============================] - ETA: 0s - loss: 0.7918\n",
            "Epoch 21: val_loss improved from 1.69169 to 1.66761, saving model to /content/best_model_256.h5\n",
            "161/161 [==============================] - 58s 360ms/step - loss: 0.7918 - val_loss: 1.6676\n",
            "Epoch 22/30\n",
            "161/161 [==============================] - ETA: 0s - loss: 0.7170\n",
            "Epoch 22: val_loss improved from 1.66761 to 1.63492, saving model to /content/best_model_256.h5\n",
            "161/161 [==============================] - 57s 355ms/step - loss: 0.7170 - val_loss: 1.6349\n",
            "Epoch 23/30\n",
            "161/161 [==============================] - ETA: 0s - loss: 0.6466\n",
            "Epoch 23: val_loss improved from 1.63492 to 1.60905, saving model to /content/best_model_256.h5\n",
            "161/161 [==============================] - 56s 346ms/step - loss: 0.6466 - val_loss: 1.6091\n",
            "Epoch 24/30\n",
            "161/161 [==============================] - ETA: 0s - loss: 0.5852\n",
            "Epoch 24: val_loss improved from 1.60905 to 1.59645, saving model to /content/best_model_256.h5\n",
            "161/161 [==============================] - 57s 357ms/step - loss: 0.5852 - val_loss: 1.5965\n",
            "Epoch 25/30\n",
            "161/161 [==============================] - ETA: 0s - loss: 0.5247\n",
            "Epoch 25: val_loss improved from 1.59645 to 1.57647, saving model to /content/best_model_256.h5\n",
            "161/161 [==============================] - 57s 351ms/step - loss: 0.5247 - val_loss: 1.5765\n",
            "Epoch 26/30\n",
            "161/161 [==============================] - ETA: 0s - loss: 0.4705\n",
            "Epoch 26: val_loss improved from 1.57647 to 1.56383, saving model to /content/best_model_256.h5\n",
            "161/161 [==============================] - 57s 352ms/step - loss: 0.4705 - val_loss: 1.5638\n",
            "Epoch 27/30\n",
            "161/161 [==============================] - ETA: 0s - loss: 0.4208\n",
            "Epoch 27: val_loss improved from 1.56383 to 1.55564, saving model to /content/best_model_256.h5\n",
            "161/161 [==============================] - 56s 350ms/step - loss: 0.4208 - val_loss: 1.5556\n",
            "Epoch 28/30\n",
            "161/161 [==============================] - ETA: 0s - loss: 0.3772\n",
            "Epoch 28: val_loss improved from 1.55564 to 1.54575, saving model to /content/best_model_256.h5\n",
            "161/161 [==============================] - 56s 347ms/step - loss: 0.3772 - val_loss: 1.5457\n",
            "Epoch 29/30\n",
            "161/161 [==============================] - ETA: 0s - loss: 0.3377\n",
            "Epoch 29: val_loss improved from 1.54575 to 1.54256, saving model to /content/best_model_256.h5\n",
            "161/161 [==============================] - 56s 346ms/step - loss: 0.3377 - val_loss: 1.5426\n",
            "Epoch 30/30\n",
            "161/161 [==============================] - ETA: 0s - loss: 0.3016\n",
            "Epoch 30: val_loss improved from 1.54256 to 1.53664, saving model to /content/best_model_256.h5\n",
            "161/161 [==============================] - 55s 345ms/step - loss: 0.3016 - val_loss: 1.5366\n",
            "63/63 [==============================] - 2s 36ms/step - loss: 1.5972\n",
            "testing loss for unit 256: 1.5971723794937134\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NlixW2SLiHhJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# because we got the best score with the larger model of the 2, I will try one more training, this time with 400 units. Anything over this would probably crash my RAM\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "big_model = create_model(rus_vocab_size, eng_vocab_size, 10, 5, 400)\n",
        "print(big_model.summary())\n",
        "filename = f'/content/best_model_unit_400.h5'\n",
        "checkpoint = ModelCheckpoint(filename, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
        "big_model.fit(train_X, train_y, epochs=30, batch_size=100, validation_data=(val_X, val_y), callbacks=[checkpoint], verbose=1)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "44gg9yfjaOQD",
        "outputId": "320c9771-12e0-4df3-d120-bd2be5a8898f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_1 (Embedding)     (None, 10, 400)           2264000   \n",
            "                                                                 \n",
            " lstm_2 (LSTM)               (None, 400)               1281600   \n",
            "                                                                 \n",
            " repeat_vector_1 (RepeatVec  (None, 5, 400)            0         \n",
            " tor)                                                            \n",
            "                                                                 \n",
            " lstm_3 (LSTM)               (None, 5, 400)            1281600   \n",
            "                                                                 \n",
            " time_distributed_1 (TimeDi  (None, 5, 3212)           1288012   \n",
            " stributed)                                                      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 6115212 (23.33 MB)\n",
            "Trainable params: 6115212 (23.33 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/30\n",
            "161/161 [==============================] - ETA: 0s - loss: 4.1780\n",
            "Epoch 1: val_loss improved from inf to 3.46015, saving model to /content/best_model_unit_400.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r161/161 [==============================] - 98s 564ms/step - loss: 4.1780 - val_loss: 3.4602\n",
            "Epoch 2/30\n",
            "161/161 [==============================] - ETA: 0s - loss: 3.2974\n",
            "Epoch 2: val_loss improved from 3.46015 to 3.20060, saving model to /content/best_model_unit_400.h5\n",
            "161/161 [==============================] - 87s 542ms/step - loss: 3.2974 - val_loss: 3.2006\n",
            "Epoch 3/30\n",
            "161/161 [==============================] - ETA: 0s - loss: 2.9519\n",
            "Epoch 3: val_loss improved from 3.20060 to 2.86406, saving model to /content/best_model_unit_400.h5\n",
            "161/161 [==============================] - 86s 534ms/step - loss: 2.9519 - val_loss: 2.8641\n",
            "Epoch 4/30\n",
            "161/161 [==============================] - ETA: 0s - loss: 2.6107\n",
            "Epoch 4: val_loss improved from 2.86406 to 2.62482, saving model to /content/best_model_unit_400.h5\n",
            "161/161 [==============================] - 86s 536ms/step - loss: 2.6107 - val_loss: 2.6248\n",
            "Epoch 5/30\n",
            "161/161 [==============================] - ETA: 0s - loss: 2.3364\n",
            "Epoch 5: val_loss improved from 2.62482 to 2.43607, saving model to /content/best_model_unit_400.h5\n",
            "161/161 [==============================] - 85s 530ms/step - loss: 2.3364 - val_loss: 2.4361\n",
            "Epoch 6/30\n",
            "161/161 [==============================] - ETA: 0s - loss: 2.1009\n",
            "Epoch 6: val_loss improved from 2.43607 to 2.27032, saving model to /content/best_model_unit_400.h5\n",
            "161/161 [==============================] - 87s 544ms/step - loss: 2.1009 - val_loss: 2.2703\n",
            "Epoch 7/30\n",
            "161/161 [==============================] - ETA: 0s - loss: 1.9018\n",
            "Epoch 7: val_loss improved from 2.27032 to 2.15575, saving model to /content/best_model_unit_400.h5\n",
            "161/161 [==============================] - 86s 531ms/step - loss: 1.9018 - val_loss: 2.1557\n",
            "Epoch 8/30\n",
            "161/161 [==============================] - ETA: 0s - loss: 1.7297\n",
            "Epoch 8: val_loss improved from 2.15575 to 2.05835, saving model to /content/best_model_unit_400.h5\n",
            "161/161 [==============================] - 86s 533ms/step - loss: 1.7297 - val_loss: 2.0584\n",
            "Epoch 9/30\n",
            "161/161 [==============================] - ETA: 0s - loss: 1.5669\n",
            "Epoch 9: val_loss improved from 2.05835 to 1.96809, saving model to /content/best_model_unit_400.h5\n",
            "161/161 [==============================] - 84s 520ms/step - loss: 1.5669 - val_loss: 1.9681\n",
            "Epoch 10/30\n",
            "161/161 [==============================] - ETA: 0s - loss: 1.4165\n",
            "Epoch 10: val_loss improved from 1.96809 to 1.88861, saving model to /content/best_model_unit_400.h5\n",
            "161/161 [==============================] - 84s 520ms/step - loss: 1.4165 - val_loss: 1.8886\n",
            "Epoch 11/30\n",
            "161/161 [==============================] - ETA: 0s - loss: 1.2732\n",
            "Epoch 11: val_loss improved from 1.88861 to 1.81589, saving model to /content/best_model_unit_400.h5\n",
            "161/161 [==============================] - 84s 521ms/step - loss: 1.2732 - val_loss: 1.8159\n",
            "Epoch 12/30\n",
            "161/161 [==============================] - ETA: 0s - loss: 1.1437\n",
            "Epoch 12: val_loss improved from 1.81589 to 1.75792, saving model to /content/best_model_unit_400.h5\n",
            "161/161 [==============================] - 98s 607ms/step - loss: 1.1437 - val_loss: 1.7579\n",
            "Epoch 13/30\n",
            "161/161 [==============================] - ETA: 0s - loss: 1.0135\n",
            "Epoch 13: val_loss improved from 1.75792 to 1.69798, saving model to /content/best_model_unit_400.h5\n",
            "161/161 [==============================] - 91s 565ms/step - loss: 1.0135 - val_loss: 1.6980\n",
            "Epoch 14/30\n",
            "161/161 [==============================] - ETA: 0s - loss: 0.8943\n",
            "Epoch 14: val_loss improved from 1.69798 to 1.64202, saving model to /content/best_model_unit_400.h5\n",
            "161/161 [==============================] - 84s 523ms/step - loss: 0.8943 - val_loss: 1.6420\n",
            "Epoch 15/30\n",
            "161/161 [==============================] - ETA: 0s - loss: 0.7875\n",
            "Epoch 15: val_loss improved from 1.64202 to 1.60689, saving model to /content/best_model_unit_400.h5\n",
            "161/161 [==============================] - 85s 529ms/step - loss: 0.7875 - val_loss: 1.6069\n",
            "Epoch 16/30\n",
            "161/161 [==============================] - ETA: 0s - loss: 0.6875\n",
            "Epoch 16: val_loss improved from 1.60689 to 1.56590, saving model to /content/best_model_unit_400.h5\n",
            "161/161 [==============================] - 83s 518ms/step - loss: 0.6875 - val_loss: 1.5659\n",
            "Epoch 17/30\n",
            "161/161 [==============================] - ETA: 0s - loss: 0.5975\n",
            "Epoch 17: val_loss improved from 1.56590 to 1.54251, saving model to /content/best_model_unit_400.h5\n",
            "161/161 [==============================] - 84s 521ms/step - loss: 0.5975 - val_loss: 1.5425\n",
            "Epoch 18/30\n",
            "161/161 [==============================] - ETA: 0s - loss: 0.5130\n",
            "Epoch 18: val_loss improved from 1.54251 to 1.50822, saving model to /content/best_model_unit_400.h5\n",
            "161/161 [==============================] - 85s 525ms/step - loss: 0.5130 - val_loss: 1.5082\n",
            "Epoch 19/30\n",
            "161/161 [==============================] - ETA: 0s - loss: 0.4408\n",
            "Epoch 19: val_loss improved from 1.50822 to 1.48828, saving model to /content/best_model_unit_400.h5\n",
            "161/161 [==============================] - 86s 534ms/step - loss: 0.4408 - val_loss: 1.4883\n",
            "Epoch 20/30\n",
            "161/161 [==============================] - ETA: 0s - loss: 0.3764\n",
            "Epoch 20: val_loss improved from 1.48828 to 1.47561, saving model to /content/best_model_unit_400.h5\n",
            "161/161 [==============================] - 84s 521ms/step - loss: 0.3764 - val_loss: 1.4756\n",
            "Epoch 21/30\n",
            "161/161 [==============================] - ETA: 0s - loss: 0.3221\n",
            "Epoch 21: val_loss improved from 1.47561 to 1.46757, saving model to /content/best_model_unit_400.h5\n",
            "161/161 [==============================] - 89s 551ms/step - loss: 0.3221 - val_loss: 1.4676\n",
            "Epoch 22/30\n",
            "161/161 [==============================] - ETA: 0s - loss: 0.2770\n",
            "Epoch 22: val_loss improved from 1.46757 to 1.46434, saving model to /content/best_model_unit_400.h5\n",
            "161/161 [==============================] - 88s 548ms/step - loss: 0.2770 - val_loss: 1.4643\n",
            "Epoch 23/30\n",
            "161/161 [==============================] - ETA: 0s - loss: 0.2361\n",
            "Epoch 23: val_loss improved from 1.46434 to 1.45596, saving model to /content/best_model_unit_400.h5\n",
            "161/161 [==============================] - 89s 554ms/step - loss: 0.2361 - val_loss: 1.4560\n",
            "Epoch 24/30\n",
            "161/161 [==============================] - ETA: 0s - loss: 0.2005\n",
            "Epoch 24: val_loss improved from 1.45596 to 1.44778, saving model to /content/best_model_unit_400.h5\n",
            "161/161 [==============================] - 82s 508ms/step - loss: 0.2005 - val_loss: 1.4478\n",
            "Epoch 25/30\n",
            "161/161 [==============================] - ETA: 0s - loss: 0.1697\n",
            "Epoch 25: val_loss did not improve from 1.44778\n",
            "161/161 [==============================] - 83s 514ms/step - loss: 0.1697 - val_loss: 1.4636\n",
            "Epoch 26/30\n",
            "161/161 [==============================] - ETA: 0s - loss: 0.1430\n",
            "Epoch 26: val_loss did not improve from 1.44778\n",
            "161/161 [==============================] - 84s 522ms/step - loss: 0.1430 - val_loss: 1.4547\n",
            "Epoch 27/30\n",
            "161/161 [==============================] - ETA: 0s - loss: 0.1219\n",
            "Epoch 27: val_loss did not improve from 1.44778\n",
            "161/161 [==============================] - 83s 515ms/step - loss: 0.1219 - val_loss: 1.4671\n",
            "Epoch 28/30\n",
            "161/161 [==============================] - ETA: 0s - loss: 0.1032\n",
            "Epoch 28: val_loss did not improve from 1.44778\n",
            "161/161 [==============================] - 83s 518ms/step - loss: 0.1032 - val_loss: 1.4675\n",
            "Epoch 29/30\n",
            "161/161 [==============================] - ETA: 0s - loss: 0.0870\n",
            "Epoch 29: val_loss did not improve from 1.44778\n",
            "161/161 [==============================] - 85s 531ms/step - loss: 0.0870 - val_loss: 1.4744\n",
            "Epoch 30/30\n",
            "161/161 [==============================] - ETA: 0s - loss: 0.0744\n",
            "Epoch 30: val_loss did not improve from 1.44778\n",
            "161/161 [==============================] - 83s 517ms/step - loss: 0.0744 - val_loss: 1.4811\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x78664dc2d120>"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "model_path = '/content/best_model_unit_400.h5'\n",
        "big_model = load_model(model_path)"
      ],
      "metadata": {
        "id": "9VDivsozMg64"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "big_model.evaluate(test_X, test_y, verbose=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rvZUhmAnszVC",
        "outputId": "0ae3d473-f123-43b5-f462-2ae120fa1b4d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "63/63 [==============================] - 4s 60ms/step - loss: 1.5248\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.5248371362686157"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "the best model seems to be the one with 400 units"
      ],
      "metadata": {
        "id": "r9CROgVxvSPg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "translation = big_model.predict(test_X, verbose=1)\n",
        "print(translation)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iR02bSL-06tH",
        "outputId": "6eda1e43-4ca7-4cd0-f061-9f199556cfd4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "63/63 [==============================] - 11s 115ms/step\n",
            "[[[4.30398011e-08 2.46817584e-12 9.94995415e-01 ... 5.96067642e-19\n",
            "   1.66117185e-12 1.04383059e-19]\n",
            "  [4.04719998e-07 2.07976880e-09 2.97707098e-04 ... 2.41531296e-13\n",
            "   1.60793001e-09 1.03913080e-12]\n",
            "  [1.49138284e-08 7.07315526e-11 4.86425888e-05 ... 2.20504343e-14\n",
            "   5.29568958e-11 2.50583889e-14]\n",
            "  [3.94827127e-03 7.95358162e-08 2.04193304e-04 ... 2.11184672e-12\n",
            "   9.50405123e-08 2.14403287e-11]\n",
            "  [9.99080777e-01 6.99312136e-11 1.09625137e-06 ... 5.84129643e-16\n",
            "   9.42962444e-11 9.92301385e-15]]\n",
            "\n",
            " [[1.92966194e-08 1.16507791e-12 9.98167038e-01 ... 4.48111461e-20\n",
            "   7.69835964e-13 5.26046157e-20]\n",
            "  [6.64056259e-08 5.68297735e-11 2.37636996e-05 ... 1.62281401e-16\n",
            "   3.97515215e-11 2.26176982e-15]\n",
            "  [8.79313757e-06 2.80010326e-09 8.57503247e-03 ... 1.56708549e-13\n",
            "   1.90689486e-09 1.05356236e-11]\n",
            "  [1.89315025e-02 4.59230414e-08 1.90693231e-06 ... 4.99096814e-12\n",
            "   4.22111768e-08 1.13668308e-09]\n",
            "  [9.99911606e-01 2.86389901e-12 5.16906411e-08 ... 4.70102789e-17\n",
            "   2.62677857e-12 8.50610814e-15]]\n",
            "\n",
            " [[3.58424842e-07 3.65617953e-12 2.15637010e-05 ... 5.57074511e-16\n",
            "   1.82741951e-12 2.01397518e-17]\n",
            "  [3.68991709e-06 5.75680545e-11 1.98717046e-07 ... 1.78525389e-11\n",
            "   3.41427026e-11 1.18560879e-12]\n",
            "  [1.65796839e-04 1.04214370e-09 2.31236257e-08 ... 1.98251406e-08\n",
            "   6.01273087e-10 1.79263804e-08]\n",
            "  [9.99774456e-01 9.61250177e-12 2.56565968e-09 ... 2.83401693e-13\n",
            "   9.38528075e-12 1.97310853e-12]\n",
            "  [9.99993384e-01 3.42628920e-13 7.48065845e-11 ... 1.16913366e-14\n",
            "   3.28391258e-13 5.41190966e-14]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[3.99332745e-09 2.36509944e-11 6.02931305e-09 ... 1.13258903e-13\n",
            "   1.63584597e-11 7.32163353e-13]\n",
            "  [6.53195775e-06 8.93294716e-10 5.50566619e-06 ... 2.22283758e-12\n",
            "   4.65722239e-10 5.83128823e-11]\n",
            "  [1.22187696e-02 2.08624362e-09 1.05198378e-05 ... 5.10909414e-12\n",
            "   1.84058035e-09 6.27562724e-10]\n",
            "  [9.96160209e-01 5.20524041e-11 1.38500047e-08 ... 3.61309053e-14\n",
            "   4.85539109e-11 2.45246792e-11]\n",
            "  [9.99934137e-01 1.53536580e-12 7.81212109e-11 ... 1.15589779e-15\n",
            "   1.63436972e-12 1.19800871e-12]]\n",
            "\n",
            " [[1.15302726e-06 3.25806471e-09 1.00972702e-05 ... 2.21979444e-13\n",
            "   3.10489057e-09 6.95223046e-12]\n",
            "  [4.09461136e-05 4.20950652e-09 2.81227659e-03 ... 2.53581959e-13\n",
            "   3.37873884e-09 3.11799501e-12]\n",
            "  [1.15679555e-04 3.96054345e-09 7.68921256e-01 ... 1.48764334e-11\n",
            "   3.20837357e-09 3.48427433e-11]\n",
            "  [5.96858971e-02 8.92290473e-08 3.49470675e-02 ... 1.94408073e-10\n",
            "   1.04354257e-07 3.05305514e-09]\n",
            "  [9.99384582e-01 1.03258319e-10 1.98537828e-06 ... 6.33203749e-14\n",
            "   1.01025376e-10 4.71669741e-13]]\n",
            "\n",
            " [[9.11485287e-08 7.84290244e-10 9.72513250e-11 ... 7.40320513e-11\n",
            "   7.21585569e-10 5.39447764e-10]\n",
            "  [9.24101187e-06 2.23968205e-10 2.26013356e-07 ... 1.80189835e-10\n",
            "   1.60363237e-10 1.83848228e-10]\n",
            "  [1.93757378e-02 9.05366893e-09 2.85361814e-08 ... 8.24416713e-09\n",
            "   9.17914011e-09 3.76744396e-08]\n",
            "  [9.99677300e-01 9.46262600e-12 4.37570380e-09 ... 8.72405718e-14\n",
            "   1.11108059e-11 1.06617818e-12]\n",
            "  [9.99969125e-01 1.11083213e-12 1.59558075e-10 ... 2.89364581e-14\n",
            "   1.51783241e-12 6.03012052e-13]]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "index_to_word = {index: word for word, index in eng_tokenizer.word_index.items()}\n"
      ],
      "metadata": {
        "id": "uf2D-xTn-Qf0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tCVLqAZB0ifl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "translations = []\n",
        "for sentence in translation:\n",
        "  output = ''\n",
        "  for word in sentence:\n",
        "    token = np.argmax(word)\n",
        "    if token == 0:\n",
        "      continue\n",
        "    else:\n",
        "      eng_word = index_to_word[token]\n",
        "    output+=eng_word\n",
        "  translations.append(output)\n",
        "\n",
        "print(translations)\n",
        "\n",
        "true = []\n",
        "for sentence in test_y:\n",
        "  output = []\n",
        "  for word in sentence:\n",
        "    token = np.argmax(word)\n",
        "    if token == 0:\n",
        "      continue\n",
        "    else:\n",
        "      eng_word = index_to_word[token]\n",
        "    output+= eng_word\n",
        "  true.append(output)\n",
        "\n",
        "\n",
        "results = pd.DataFrame({'true': true, 'trans': translations})\n",
        "\n"
      ],
      "metadata": {
        "id": "EK7G-IA8_On_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results.to_csv('/content/results_string.csv', index=False)"
      ],
      "metadata": {
        "id": "SULLNnYIWVl4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "results = pd.read_csv('/content/results.csv')\n"
      ],
      "metadata": {
        "id": "5fBm2ev0GVh_"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "I have an older version of the results df downloaded and had to restart the runtime, so I will do some preprocessing again."
      ],
      "metadata": {
        "id": "sAM-xvlAJsP3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import ast\n",
        "results['trans'] = results['trans'].apply(ast.literal_eval)\n",
        "results['true'] = results['true'].apply(ast.literal_eval)"
      ],
      "metadata": {
        "id": "eiUY24LBZYCy"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results['trans']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "USHLtvP5KUJF",
        "outputId": "e82a5cd0-5423-4ff7-b68e-b951d918f92b"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0       [i, wanted, to, sing, ]\n",
              "1         [i, hope, you, die, ]\n",
              "2         [you, are, young, , ]\n",
              "3          [it, is, too, hot, ]\n",
              "4          [we, will, wait, , ]\n",
              "                 ...           \n",
              "1981     [i, am, a, sculptor, ]\n",
              "1982       [give, me, half, , ]\n",
              "1983           [go, go, go, , ]\n",
              "1984    [what, will, i, have, ]\n",
              "1985      [open, a, window, , ]\n",
              "Name: trans, Length: 1986, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trans = results['trans']\n",
        "\n",
        "translations = []\n",
        "for row in trans:\n",
        "  string = ''\n",
        "  for token in row:\n",
        "    if token == '':\n",
        "      continue\n",
        "    if token == 'ca': #in the preprocessing process I accidently replaced can't with ca not\n",
        "      token = 'can'\n",
        "    if token == 'wo': #in the preprocessing process I accidently replaced won't with wo not\n",
        "      token = 'will'\n",
        "\n",
        "    # we will get rid of some clearly equivlant translations, such as 'a' and 'the' being equivlant in russian\n",
        "\n",
        "    if token == 'a':\n",
        "      token = 'the'\n",
        "\n",
        "    string+= token + ' '\n",
        "  translations.append(string)\n",
        "\n",
        "true = results['true']\n",
        "\n",
        "trues = []\n",
        "for row in true:\n",
        "  string = ''\n",
        "  for token in row:\n",
        "    if token == '':\n",
        "      continue\n",
        "    if token == 'ca': #in the preprocessing process I accidently replaced can't with ca not\n",
        "      token = 'can'\n",
        "    if token == 'wo': #in the preprocessing process I accidently replaced won't with wo not\n",
        "      token = 'will'\n",
        "\n",
        "    # we will get rid of some clearly equivlant translations, such as 'a' and 'the' being equivlant in russian\n",
        "\n",
        "    if token == 'a':\n",
        "      token = 'the'\n",
        "\n",
        "    string+= token + ' '\n",
        "  trues.append(string)\n",
        "\n",
        "trues = [sentence[:-1] for sentence in trues] # to get rid of extra spasce at the end\n",
        "translations = [sentence[:-1] for sentence in translations]"
      ],
      "metadata": {
        "id": "sQ7NhTdNG6IC"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trues[30:40]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6wOJqlgtOEFS",
        "outputId": "0addfded-74d3-4957-ec88-4f167cd3d38d"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['tom had the gun',\n",
              " 'i am stubborn',\n",
              " 'we can make it',\n",
              " 'i got mad',\n",
              " 'tom was angry',\n",
              " 'i was not alone',\n",
              " 'i read his book',\n",
              " 'cover your legs',\n",
              " 'i sold the book',\n",
              " 'stop quibbling']"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "translations[30:40]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9MKeNrncOG7r",
        "outputId": "eed328c3-a53f-4f6b-e86b-69a3f87c27c0"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['tom had the gun',\n",
              " 'i am contagious',\n",
              " 'we will it it',\n",
              " 'i was myself',\n",
              " 'tom was angry',\n",
              " 'i was not alone',\n",
              " 'i read it book',\n",
              " 'cover your feet',\n",
              " 'i took the book',\n",
              " 'toms famous']"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install evaluate"
      ],
      "metadata": {
        "id": "FN_dikNiS9HE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import evaluate\n",
        "bleu = evaluate.load('bleu')\n",
        "predictions = translations\n",
        "references = trues\n",
        "results = bleu.compute(predictions=predictions, references=references,\n",
        "          max_order = 2) # the sentences are very short so we will only use 2 n-grams\n",
        "print(results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CkwuDjtISiDN",
        "outputId": "3cd4953a-30e2-414e-a40a-67715caa9013"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'bleu': 0.5969121417304739, 'precisions': [0.695322376738306, 0.526715799170889], 'brevity_penalty': 0.9863456581662835, 'length_ratio': 0.9864380358534685, 'translation_length': 6328, 'reference_length': 6415}\n"
          ]
        }
      ]
    }
  ]
}